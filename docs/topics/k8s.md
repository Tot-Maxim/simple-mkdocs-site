### [Назад к оглавлению](../index.md)

### [Kubernetes](https://kubernetes.io/docs/home/)

### [Helm](Helm.md)

# Kubernetes

- [Kubernetes](#kubernetes)
    - [Kubernetes 101](#kubernetes-101)
    - [Кластер и Архитектура](#cluster-and-architecture)
      - [Kubelet](#kubelet)
      - [Команды узлов](#команды-узлов)
    - [Pods](#pods)
      - [Статические Pods](#статические-pods)
      - [Команды Pods](#команды-pods)
      - [Поиск и отладка Pods](#поиск-и-отладка-pods)
    - [Метки и Селекторы](#метки-и-селекторы)
    - [Развертывания](#развертывания)
      - [Команды Развертываний](#команды-развертываний)
    - [Сервисы](#сервисы)
    - [Ingress](#ingress)
    - [ReplicaSets](#replicasets)
    - [DaemonSet](#daemonset)
      - [DaemonSet - команды](#daemonset---команды)
    - [StatefulSet](#statefulset)
    - [Хранилище](#хранение)
      - [Тома](#тома)
    - [Сетевые технологии](#сеть)
    - [Сетевые Политики](#сетевые-политики)
    - [etcd](#etcd)
    - [Пространства имен](#namespaces)
      - [Команды пространств имен](#namespaces---commands)
      - [Квота ресурсов](#resources-quota)
    - [Операторы](#operators)
    - [Секреты](#секреты)
    - [Тома](#тома)
    - [Контроль доступа](#Сеть)
    - [Шаблоны](#patterns)
    - [CronJob](#cronjob)
    - [Прочее](#misc)
    - [Gatekeeper](#gatekeeper)
    - [Тестирование Политики](#policy-testing)
    - [Безопасность](#безопасность)
    - [Сценарии устранения неисправностей](#сценарии-устранения-неполадок)
    - [Контроллеры](#контроллеры)
    - [Планировщик](#планировщик)
      - [Аффинность узлов](#аффинность-узла)
    - [Модификаторы](#модификаторы)
    - [Ограничения ресурсов](#ограничения-ресурсов)
      - [Ограничения ресурсов - команды](#resources-limits---commands)
    - [Мониторинг](#мониторинг)
    - [Kustomize](#kustomize)
    - [Стратегии развертывания](#стратегии-развертывания)
    - [Сценарии](#сценарии)

### Kubernetes 101

<details>
<summary>Что такое Kubernetes? Почему организации его используют?</summary><br><b>

Kubernetes - это система с открытым исходным кодом, которая дает пользователям возможность управлять, масштабировать и развертывать контейнеризованные приложения.

Чтобы понять, для чего нужен Kubernetes, давайте рассмотрим несколько примеров:

* Вы хотите запустить определенное приложение в контейнере в нескольких разных местах и синхронизировать изменения между всеми этими местами, независимо от того, где они работают
* Выполнение обновлений и изменений на сотнях контейнеров
* Обработка случаев, когда текущая нагрузка требует увеличения (или уменьшения) масштабов

</b></details>

<details>
<summary>Когда или почему НЕЛЬЗЯ использовать Kubernetes?</summary><br><b>

  - Если вы управляете низкоуровневой инфраструктурой или серверным оборудованием, Kubernetes, вероятно, не то, что вам нужно или хотите
  - Если вы небольшая команда (например, менее 20 инженерных сотрудников), работающая с менее чем дюжиной контейнеров, Kubernetes может быть чрезмерным (даже если вам нужно масштабирование, развертывание обновлений и т.д.). Вы все еще можете воспользоваться преимуществами использования управляемого Kubernetes, но вам определенно стоит обдумать это тщательно, прежде чем принимать решение о его внедрении.

</b></details>

<details>
<summary>Каковы некоторые особенности Kubernetes?</summary><br><b>

  - Самовосстановление: Kubernetes использует проверки состояния для мониторинга контейнеров и выполняет определенные действия при сбоях или других типах событий, таких как перезапуск контейнера
  - Балансировка нагрузки: Kubernetes может распределять и/или балансировать запросы к приложениям, работающим в кластере, на основе состояния Pods, работающих с приложением
  - Операторы: упакованные приложения Kubernetes, которые могут использовать API кластера для обновления своего состояния и триггеров действий, основанных на событиях и изменении состояния приложения
  - Автоматическое развертывание: Постепенное развертывание обновлений приложений и поддержка отката в случае, если что-то пойдет не так
  - Масштабирование: Горизонтальное масштабирование на основе различных параметров состояния и пользовательских определенных критериев
  - Секреты: наличие механизма для безопасного хранения имен пользователей, паролей и конечных точек служб, где не каждый пользователь кластера сможет их просмотреть

</b></details>

<details>
<summary>Какие объекты Kubernetes существуют?</summary><br><b>

  * Pod
  * Service
  * ReplicationController
  * ReplicaSet
  * DaemonSet
  * Namespace
  * ConfigMap
  ...
</b></details>

<details>
<summary>Какие поля обязательны для любого объекта Kubernetes?</summary><br><b>

metadata, kind и apiVersion

</b></details>

<details>
<summary>Что такое kubectl?</summary><br><b>

Kubectl - это инструмент командной строки Kubernetes, который позволяет вам выполнять команды против кластеров Kubernetes. Например, вы можете использовать kubectl для развертывания приложений, инспекции и управления ресурсами кластера, а также для просмотра журналов.

</b></details>

<details>
<summary>Какие объекты Kubernetes вы обычно используете при развертывании приложений в Kubernetes?</summary><br><b>

* Deployment - создает Pods и следит за ними
* Service: направляет внутренний трафик на Pods
* Ingress: направляет внешний трафик из-за пределов кластера

</b></details>

<details>
<summary>Почему в Kubernetes нет такой команды? <code>kubectl get containers</code></summary><br><b>

Потому что контейнер не является объектом Kubernetes. Наименьшая единица объекта в Kubernetes - это Pod. В одном Pod вы можете найти один или несколько контейнеров.

</b></details>

<details>
<summary>Какие действия или операции вы считаете наилучшими практиками в отношении Kubernetes?</summary><br><b>

  - Всегда убедитесь, что файлы YAML Kubernetes действительны. Рекомендуется применять автоматизированные проверки и пайплайны.
  - Всегда указывайте запросы и ограничения, чтобы предотвратить ситуацию, когда контейнеры используют всю память кластера, что может привести к проблеме OOM
  - Указывайте метки для логической группировки Pods, Развертываний и т.д. Используйте метки для определения Типа приложения, например, среди прочего

</b></details>

### Кластер и Архитектура

<details>
<summary>Что такое кластер Kubernetes?</summary><br><b>

Определение Red Hat: "Кластер Kubernetes - это набор узлов, на которых работают контейнеризованные приложения. Если вы запускаете Kubernetes, вы запускаете кластер.
По минимуму, кластер должен содержать рабочий узел и главный узел."

Подробнее читайте [здесь](https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-cluster)
</b></details>

<details>
<summary>Что такое узел?</summary><br><b>

Узел - это виртуальная или физическая машина, которая служит рабочим узлом для запуска приложений.<br>
Рекомендуется иметь как минимум 3 узла в производственной среде.
</b></details>

<details>
<summary>За что отвечает главный узел?</summary><br><b>

Главный узел координирует все рабочие процессы в кластере:

* Планирование приложений
* Управление желаемым состоянием
* Развертывание новых обновлений

</b></details>

<details>
<summary>Кратко опишите на высоком уровне, что происходит, когда вы выполняете <code>kubectl get nodes</code></summary><br><b>

1. Ваш пользователь аутентифицируется
2. Запрос проверяется kube-apiserver
3. Данные извлекаются из etcd
</b></details>

<details>
<summary>Истина или Ложь? Каждый кластер должен иметь 0 или более главных узлов и как минимум 1 рабочий узел</summary><br><b>

Ложь. Кластер Kubernetes состоит как минимум из 1 главного узла и может иметь 0 рабочих (хотя это не будет очень полезно...)

</b></details> 

<details>
<summary>Какие компоненты есть у главного узла (также известного как управляющая плоскость)?</summary><br><b>

  * API Сервер - API Kubernetes. Все компоненты кластера взаимодействуют через него
  * Планировщик - назначает приложению рабочий узел, на котором оно может работать
  * Контроллер Менеджер - обслуживание кластера (репликации, сбои узлов и т.д.)
  * etcd - хранит настройки кластера

</b></details>

<details>
<summary>Какие компоненты есть у рабочего узла (также известного как плоскость данных)?</summary><br><b>

  * Kubelet - агент, ответственный за связь узла с главным узлом.
  * Kube-proxy - балансирует нагрузку между компонентами приложений
  * Рuntime контейнеров - движок для запуска контейнеров (Podman, Docker, ...)

</b></details>

<details>
<summary>Поставьте компоненты справа в нужное место на изображении<br>
<img src="images/cluster_architecture_exercise.png"/>
</summary><br><b>
<img src="images/cluster_architecture_solution.png"/>

</b></details>

<details>
<summary>Вы управляете несколькими кластерами Kubernetes. Как быстро переключаться между кластерами, используя kubectl?</summary><br><b>

`kubectl config use-context`
</b></details>

<details>
<summary>Как предотвратить высокое использование памяти в вашем кластере Kubernetes и возможные проблемы такие как утечка памяти и OOM?</summary><br><b>

Применяйте запросы и ограничения, особенно для сторонних приложений (где неопределенность еще больше)
</b></details>

<details>
<summary>У вас есть опыт развертывания кластера Kubernetes? Если да, можете ли вы описать процесс на высоком уровне?</summary><br><b>

1. Создайте несколько экземпляров, которые вы будете использовать в качестве узлов/рабочих Kubernetes. Создайте также экземпляр, который будет выступать в качестве Главного. Экземпляры могут быть размещены в облаке или виртуальными машинами на серверном оборудовании.
2. Обеспечьте наличие центра сертификации, который будет использоваться для создания TLS сертификатов для различных компонентов кластера Kubernetes (kubelet, etcd и т.д.)
  1. Сгенерируйте сертификат и закрытый ключ для различных компонентов
3. Сгенерируйте kubeconfigs, чтобы различные клиенты Kubernetes могли находить API серверы и аутентифицироваться.
4. Сгенерируйте ключ шифрования, который будет использоваться для шифрования данных кластера
5. Создайте кластер etcd
</b></details>

<details>
<summary>Какая команда перечисляет все типы объектов в кластере?</summary><br><b>

`kubectl api-resources`
</b></details>

<details>
<summary>Что делает команда <code>kubectl get componentstatus</code>?</summary><br><b>

Выводит статус каждого из компонентов управляющей плоскости.
</b></details>

#### Kubelet

<details>
<summary>Что произойдет с работающими pods, если вы остановите Kubelet на рабочих узлах?</summary><br><b>

Когда вы останавливаете службу kubelet на рабочем узле, он больше не сможет общаться с Kubernetes API сервером. В результате узел будет помечен как NotReady, а pods, работающие на этом узле, будут помечены как Unknown. Затем управляющая плоскость Kubernetes попытается переназначить pods на другие доступные узлы в кластере. 
</b></details>

#### Команды Узлов

<details>
<summary>Выполните команду, чтобы просмотреть все узлы кластера</summary><br><b>

`kubectl get nodes`

Примечание: Вы можете создать псевдоним (`alias k=kubectl`) и привыкнуть к `k get no`
</b></details>

<details>
<summary>Создайте список всех узлов в формате JSON и сохраните его в файл с именем "some_nodes.json"</summary><br><b>

`k get nodes -o json > some_nodes.json`
</b></details>

<details>
<summary>Проверьте, какие метки есть у одного из ваших узлов в кластере</summary><br><b>

`k get no minikube --show-labels`
</b></details>

### Pods

<details>
<summary>Объясните, что такое Pod</summary><br><b>

Pod - это группа из одного или более контейнеров, с общей памятью и сетевыми ресурсами, а также спецификацией, как запускать контейнеры.

Pods - это наименьшие развертываемые единицы вычислений, которые вы можете создать и управлять в Kubernetes. 

</b></details>

<details>
<summary>Разверните pod с именем "my-pod", используя образ nginx:alpine</summary><br><b>

`kubectl run my-pod --image=nginx:alpine`

Если вы новичок в Kubernetes, вы должны знать, что это не распространенный способ запуска Pods. Обычно Pods запускаются через Deployment, который в свою очередь запускает Pod(ы).

Кроме того, Pods и/или Развертывания обычно определяются в файлах, а не выполняются непосредственно с использованием только аргументов CLI.
</b></details>

<details>
<summary>Каковы ваши мысли по поводу "Pods не предназначены для непосредственного создания"?</summary><br><b>

Pods обычно действительно не создаются непосредственно. Вы заметите, что Pods обычно создаются как часть других сущностей, таких как Развертывания или ReplicaSets.

Если Pod умирает, Kubernetes не восстановит его. Поэтому гораздо полезнее, например, определить ReplicaSets, которые будут обеспечивать, чтобы заданное количество Pods всегда функционировало, даже после того, как определенный Pod умрет.
</b></details>

<details>
<summary>Сколько контейнеров может содержать pod?</summary><br><b>

Pod может включать несколько контейнеров, но в большинстве случаев это, вероятно, будет один контейнер на pod.

Существуют определенные шаблоны, когда имеет смысл запускать более одного контейнера, например, "шаблон побочного контейнера", когда вы можете захотеть выполнить логирование или некоторые другие операции, которые выполняются другим контейнером, работающим с вашим приложением в одном Pod.
</b></details>

<details>
<summary>Какие существуют случаи использования для запуска нескольких контейнеров в одном pod?</summary><br><b>

Веб-приложение с отдельными (в своих контейнерах) компонентами/адаптерами логирования и мониторинга - это один из примеров.<br>
CI/CD пайплайн (например, с использованием Tekton) может запускать несколько контейнеров в одном Pod, если Задача содержит несколько команд.
</b></details>

<details>
<summary>Каковы возможные фазы Pod?</summary><br><b>

  * Running - Pod связан с узлом и хотя бы один контейнер запущен
  * Failed/Error - По крайней мере, один контейнер в Pod завершился с ошибкой
  * Succeeded - Каждый контейнер в Pod завершился успешно
  * Unknown - Состояние Pod не может быть получено
  * Pending - Контейнеры еще не работают (возможно, образы все еще загружаются или pod еще не был запланирован)
</b></details>

<details>
<summary>Истина или Ложь? По умолчанию pods изолированы. Это означает, что они не могут принимать трафик из любого источника</summary><br><b>

Ложь. По умолчанию pods не изолированы = pods принимают трафик из любого источника.
</b></details>

<details>
<summary>Истина или Ложь? Фаза "Pending" означает, что Pod еще не был принят кластером Kubernetes, поэтому планировщик не может его запустить, пока он не будет принят</summary><br><b>

Ложь. "Pending" - это после того, как Pod был принят кластером, но контейнер не может запуститься по разным причинам, например, из-за того, что образы еще не загружены.
</b></details>

<details>
<summary>Истина или Ложь? Один Pod может быть разделен на несколько узлов</summary><br><b>

Ложь. Один Pod может работать только на одном узле.
</b></details>

<details>
<summary>Вы запускаете pod и видите статус <code>ContainerCreating</code></summary><br><b>
</b></details>

<details>
<summary>Истина или Ложь? Том, определенный в Pod, может быть доступен всеми контейнерами этого Pod</summary><br><b>

Истина.
</b></details>

<details>
<summary>Что происходит, когда вы запускаете Pod с помощью kubectl?</summary><br><b>

1. Kubectl отправляет запрос на API сервер (kube-apiserver) для создания Pod
   1. В процессе пользователь аутентифицируется, и запрос проверяется.
   2. etcd обновляется данными
2. Планировщик обнаруживает, что есть неподписанный Pod, следя за API сервером (kube-apiserver)
3. Планировщик выбирает узел для назначения Pod
   1. etcd обновляется этой информацией
4. Планировщик обновляет API сервер о том, какой узел он выбрал
5. Kubelet (который также отслеживает API сервер) замечает, что на том узле, на котором он работает, назначен Pod, и этот Pod не работает
6. Kubelet отправляет запрос контейнерному движку (например, Docker) на создание и запуск контейнеров
7. Kubelet отправляет обновление на API сервер (уведомляя его, что Pod запущен)
   1. etcd обновляется API сервером снова
</b></details>

<details>
<summary>Как подтвердить, что контейнер работает после выполнения команды <code>kubectl run web --image nginxinc/nginx-unprivileged</code></summary><br><b>

* Когда вы выполняете `kubectl describe pods <POD_NAME>`, он сообщит, работает ли контейнер:
`Status:       Running`
* Выполните команду внутри контейнера: `kubectl exec web -- ls`
</b></details>

<details>
<summary>После выполнения <code>kubectl run database --image mongo</code> вы видите статус "CrashLoopBackOff". Что могло пойти не так и что вы делаете для подтверждения?</summary><br><b>

"CrashLoopBackOff" означает, что Pod запускается, завершает работу, снова запускается... и так повторяется.<br>
Существует множество различных причин появления этой ошибки - недостаток разрешений, неверная конфигурация init-контейнера, проблема с подключением постоянного тома и т.д.

Один из способов проверить, почему это произошло, - выполнить `kubectl describe po <POD_NAME>` и посмотреть на код выхода

```
 Last State:     Terminated
   Reason:       Error
   Exit Code:    100
```

Еще один способ выяснить, что происходит, - выполнить `kubectl logs <POD_NAME>`. Это предоставит нам журналы от контейнеров, работающих в этом Pod.
</b></details>

<details>
<summary>Объясните назначение следующих строк

```
livenessProbe:
  exec:
    command:
    - cat
    - /appStatus
  initialDelaySeconds: 10
  periodSeconds: 5
```
</summary><br><b>

Эти строки используют `проверку жизнеспособности`. Она используется для перезапуска контейнера, когда он достигает нежелательного состояния.<br>
В данном случае, если команда `cat /appStatus` завершится с ошибкой, Kubernetes завершит работу контейнера и применит политику перезапуска. `initialDelaySeconds: 10` означает, что Kubelet будет ждать 10 секунд перед тем, как запустить команду/проверку в первый раз. С этого момента она будет выполняться каждые 5 секунд, как указано в `periodSeconds`
</b></details>

<details>
<summary>Объясните назначение следующих строк

```
readinessProbe:
      tcpSocket:
        port: 2017
      initialDelaySeconds: 15
      periodSeconds: 20
```
</summary><br><b>

Они определяют проверку готовности, при которой Pod не будет помечен как "Готов" до тех пор, пока не станет возможным подключение к порту 2017 контейнера. Первая проверка/проверка начнется через 15 секунд после того, как контейнер начнет работать, и будет продолжать выполняться каждые 20 секунд, пока не удастся подключиться к определенному порту.
</b></details>

<details>
<summary>Что означает статус "ErrImagePull" Pod?</summary><br><b>

Не удалось загрузить образ, указанный для запуска контейнеров. Это может произойти, если клиент не аутентифицирован, например.<br>
Больше деталей можно получить с помощью `kubectl describe po <POD_NAME>`.
</b></details>

<details>
<summary>Что происходит, когда вы удаляете Pod?</summary><br><b>

1. Сигнал `TERM` отправляется для завершения основных процессов внутри контейнеров данного Pod
2. Каждому контейнеру дается период времени 30 секунд для корректного завершения процессов
3. Если период ожидания истечет, будет использован сигнал `KILL` для принудительного завершения процессов и контейнеров
</b></details>

<details>
<summary>Объясните проверки жизнеспособности</summary><br><b>

Проверки жизнеспособности - это полезный механизм, используемый для перезапуска контейнера, когда определенная проверка/проверка, заданная пользователем, завершилась неудачей.<br>
Например, пользователь может определить, что команда `cat /app/status` будет выполняться каждые X секунд, и как только эта команда завершится с ошибкой, контейнер будет перезапущен.

Вы можете прочитать больше об этом на [kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes)
</b></details>

<details>
<summary>Объясните проверки готовности</summary><br><b>

Проверки готовности используются Kubelet для того, чтобы определить, когда контейнер готов начать работать, принимая трафик.<br>
Например, проверка готовности может заключаться в подключении к порту 8080 на контейнере. Как только Kubelet удается подключиться, Pod помечается как готовый

Вы можете прочитать больше об этом на [kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes)
</b></details>

<details>
<summary>Как статус проверки готовности влияет на Сервисы при их объединении?</summary><br><b>

Только контейнеры, которых состояние установлено в Успех, смогут получать запросы, отправленные к Сервису.
</b></details>

<details>
<summary>Почему в большинстве случаев распространено иметь только один контейнер на Pod?</summary><br><b>

Одна из причин заключается в том, что это затрудняет масштабирование, когда необходимо масштабировать только один из контейнеров в данном Pod.
</b></details>

<details>
<summary>Истина или Ложь? Как только Pod назначен рабочему узлу, он будет работать только на этом узле, даже если он в какой-то момент выйдет из строя и запустит новый Pod</summary><br><b>

Истина.
</b></details>

<details>
<summary>Истина или Ложь? Каждый Pod при создании получает свой собственный публичный IP-адрес</summary><br><b>

Ложь. Каждый Pod получает IP-адрес, но внутренний и не является общедоступным.

Чтобы сделать Pod доступным извне, нам нужно использовать объект, называемый Сервисом в Kubernetes.
</b></details>

#### Статические Pods

<details>
<summary>Что такое статические Pods?</summary><br><b>

[Kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/): "Статические Pods управляются напрямую демоном kubelet на конкретном узле, без наблюдения со стороны API сервера. В отличие от Pods, управляемых управляющей плоскостью (например, Развертыванием); вместо этого kubelet отслеживает каждый статический Pod (и перезапускает его, если он выйдет из строя)."
</b></details>

<details>
<summary>Истина или Ложь? Точно так же, как есть "Статические Pods", существуют и другие статические ресурсы, такие как "развертывания" и "репликации"</summary><br><b>

Ложь.
</b></details>

<details>
<summary>Какие существуют случаи использования для использования статических Pods?</summary><br><b>

Одним из очевидных случаев использования является запуск Pods управляющей плоскости - запуск Pods, таких как kube-apiserver, планировщик и т.д. Они должны работать и функционировать независимо от того, работают ли некоторые компоненты кластера или нет, и они должны запускаться на определенных узлах кластера.
</b></details>

<details>
<summary>Как идентифицировать, какие Pods являются статическими Pods?</summary><br><b>

Суффикс Pods - это то же самое, что и название узлов, на которых они работают.
TODO: проверьте, всегда ли это так.
</b></details>

<details>
<summary>Какой из следующих не является статическим podом?:

* kube-scheduler
* kube-proxy
* kube-apiserver
</summary><br><b>

kube-proxy - это DaemonSet (так как он должен быть представлен на каждом узле в кластере). Нет одного конкретного узла, на котором он обязан работать.
</b></details>

<details>
<summary>Где находятся манифесты статических Pods?</summary><br><b>

Чаще всего это /etc/kubernetes/manifests, но вы можете проверить с помощью `grep -i static /var/lib/kubelet/config.yaml`, чтобы найти значение `statisPodsPath`.

Возможно, что ваша конфигурация находится по другому пути. Чтобы проверить, выполните `ps -ef | grep kubelet` и посмотрите, каково значение аргумента --config процесса `/usr/bin/kubelet`.

Ключевым моментом для определения пути статических Pods является `staticPodPath`. Если ваша конфигурация находится в `/var/lib/kubelet/config.yaml`, вы можете выполнить `grep staticPodPath /var/lib/kubelet/config.yaml`.
</b></details>

<details>
<summary>Опишите, как вы бы удалили статический Pod</summary><br><b>

Найдите каталог статических Pods (посмотрите `staticPodPath` в конфигурационном файле kubelet).

Перейдите в этот каталог и удалите манифест/определение статического Pod (`rm <STATIC_POD_PATH>/<POD_DEFINITION_FILE>`)
</b></details>

#### Команды Pods

<details>
<summary>Как узнать, на каком рабочем узле распределены Pods? Другими словами, как проверить, на каком узле работает определённый Pod?</summary><br><b>

`kubectl get pods -o wide`
</b></details>

<details>
<summary>Как удалить pod?</summary><br><b>

`kubectl delete pod pod_name`
</b></details>

<details>
<summary>Перечислите все pods с меткой "env=prod"</summary><br><b>

`k get po -l env=prod`

Чтобы подсчитать их: `k get po -l env=prod --no-headers | wc -l`
</b></details>

<details>
<summary>Как перечислить pods в текущем пространстве имен?</summary><br><b>

`kubectl get po`
</b></details>

<details>
<summary>Как просмотреть все pods, работающие во всех пространствах имен?</summary><br><b>

`kubectl get pods --all-namespaces`
</b></details>

#### Поиск и отладка Pods

<details>
<summary>Вы пытаетесь запустить Pod, но он находится в состоянии "Pending". Какова может быть причина?</summary><br><b>

Одной из возможных причин является то, что планировщик, который должен назначать Pods на узлы, не работает. Чтобы проверить это, вы можете выполнить `kubectl get po -A | grep scheduler` или проверить непосредственно в пространстве `kube-system`.
</b></details>

<details>
<summary>Что делает команда <code>kubectl logs [pod-name]</code>?</summary><br><b>

Печатает журналы для контейнера в Pod.
</b></details>

<details>
<summary>Что делает команда <code>kubectl describe pod [pod name]</code>?</summary><br><b>

Показывает детали конкретного ресурса или группы ресурсов.
</b></details>

<details>
<summary>Создать статический pod с изображением <code>python</code>, который выполняет команду <code>sleep 2017</code></summary><br><b>

Сначала перейдите в каталог, отслеживаемый kubelet для создания статического pod: `cd /etc/kubernetes/manifests` (вы можете проверить путь, прочитав конфигурационный файл kubelet)

Теперь создайте определение/манифест в этом каталоге
`k run some-pod --image=python --command sleep 2017 --restart=Never --dry-run=client -o yaml > static-pod.yaml`
</b></details>

### Метки и Селекторы

<details>
<summary>Объясните метки</summary><br><b>

[Kubernetes.io](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/): "Метки - это пары ключ/значение, которые прикрепляются к объектам, таким как pods. Метки предназначены для указания идентифицирующих атрибутов объектов, которые имеют значение и актуальны для пользователей, но не подразумевают семантики в ядре системы. Метки могут использоваться для организации и выбора подмножеств объектов. Метки могут быть прикреплены к объектам в момент создания и затем добавлены и изменены в любое время. У каждого объекта может быть набор определенных меток ключ/значение. Каждый ключ должен быть уникальным для данного объекта."
</b></details>

<details>
<summary>Объясните селекторы</summary><br><b>

[Kubernetes.io](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors): "В отличие от имен и UIDs, метки не обеспечивают уникальность. В общем, мы ожидаем, что многие объекты будут носить одни и те же метки(ы).

С помощью селектора меток клиент/пользователь может идентифицировать набор объектов. Селектор меток - это основной примитив группировки в Kubernetes.

API в данный момент поддерживает два типа селекторов: основанные на равенстве и основанные на наборе. Селектор меток может состоять из нескольких требований, которые разделены запятыми. В случае нескольких требований все должны удовлетворяться, поэтому запятая действует как логический оператор И (&&)."
</b></details>

<details>
<summary>Приведите некоторые реальные примеры того, как используются метки</summary><br><b>

* Можно использовать планировщиком, чтобы разместить определенные Pods (с определенными метками) на конкретных узлах
* Используется ReplicaSets для отслеживания pods, которые должны быть масштабированы
</b></details>

<details>
<summary>Что такое Аннотации?</summary><br><b>

[Kubernetes.io](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/): "Вы можете использовать аннотации Kubernetes для прикрепления произвольной неидентифицирующей метаданных к объектам. Клиенты, такие как инструменты и библиотеки, могут извлекать эти метаданные."
</b></details>

<details>
<summary>Как аннотации отличаются от меток?</summary><br><b>

[Kuberenets.io](Метки могут использоваться для выбора объектов и для поиска коллекций объектов, которые соответствуют определенным условиям. В отличие от этого, аннотации не используются для идентификации и выбора объектов. Метаданные в аннотации могут быть маленькими или большими, структурированными или неструктурированными и могут включать символы, не допускающиеся метками.): "Метки могут использоваться для выбора объектов и для поиска коллекций объектов, которые соответствуют определенным условиям. В отличие от этого, аннотации не используются для идентификации и выбора объектов. Метаданные в аннотации могут быть маленькими или большими, структурированными или неструктурированными и могут включать символы, не допускаемые метками."
</b></details>

<details>
<summary>Как просмотреть журналы контейнера, работающего в Pod?</summary><br><b>

`k logs POD_NAME`
</b></details>

<details>
<summary>В Pod под названием "some-pod" два контейнера. Что произойдет, если вы выполните <code>kubectl logs some-pod</code>?</summary><br><b>

Это не сработает, потому что в Pod два контейнера, и вам нужно будет указать один из них с помощью `kubectl logs POD_NAME -c CONTAINER_NAME`
</b></details>

### Развертывания

<details>
<summary>Что такое "Развертывание" в Kubernetes?</summary><br><b>

Развертывание Kubernetes используется для указания Kubernetes, как создать или изменить экземпляры Pods, которые содержат контейнеризованное приложение.
Развертывания могут масштабировать количество псевдоподов, позволять развертывание обновленного кода контролируемым образом или откатиться к более ранней версии развертывания, если это необходимо. 

Развертывание - это декларативное заявление о желаемом состоянии для Pods и Replica Sets.
</b></details>

<details>
<summary>Как создать развертывание с изображением "nginx:alpine"?</code></summary><br><b>

`kubectl create deployment my-first-deployment --image=nginx:alpine`

ИЛИ

```
cat << EOF | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
```
</b></details>

<details>
<summary>Как проверить, что развертывание было создано?</code></summary><br><b>

`kubectl get deployments` или `kubectl get deploy`

Эта команда перечисляет все объекты Развертывания, созданные и существующие в кластере. Это не означает, что развертывания готовы и работают. Это можно проверить по столбцам "READY" и "AVAILABLE".
</b></details>

<details>
<summary>Как редактировать развертывание?</code></summary><br><b>

`kubectl edit deployment <DEPLOYMENT_NAME>`
</b></details>

<details>
<summary>Что происходит после редактирования развертывания и изменения изображения?</summary><br><b>

Pod будет завершен, и будет создан другой, новый Pod.

Также, при просмотре ReplicaSet, вы увидите, что старая реплика не имеет никаких Pods, и создается новая ReplicaSet.
</b></details>

<details>
<summary>Как удалить развертывание?</summary><br><b>

Один из способов - указать имя развертывания: `kubectl delete deployment [deployment_name]`

Другой способ - использовать конфигурационный файл развертывания: `kubectl delete -f deployment.yaml`
</b></details>

<details>
<summary>Что происходит, когда вы удаляете развертывание?</summary><br><b>

Pod, связанный с развертыванием, будет завершен, и ReplicaSet будет удален.
</b></details>

<details>
<summary>Что происходит за кулисами, когда вы создаете объект Развертывания?</summary><br><b>

Следующее происходит, когда вы выполняете `kubectl create deployment some_deployment --image=nginx`

1. HTTP-запрос отправляется на сервер API Kubernetes кластера для создания нового развертывания
2. Создается новый объект Pod и назначается одному из рабочих узлов
3. Kublet на рабочем узле уведомляет движок контейнера о необходимости загрузить образ из реестра
4. Создается новый контейнер с использованием только что загруженного образа
</b></details>

<details>
<summary>Как сделать приложение доступным в частной или внешней сети?</summary><br><b>

Используя Сервис.
</b></details>

<details>
<summary>Можно ли использовать Развертывание для работы с состоянием приложений?</summary><br><b>
</b></details>

<details>
<summary>Исправьте следующий манифест развертывания

```yaml
apiVersion: apps/v1
kind: Deploy
metadata:
  creationTimestamp: null
  labels:
    app: dep
  name: dep
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dep
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: dep
    spec:
      containers:
      - image: redis
        name: redis
        resources: {}
status: {}
```
</summary><br><b>

Измените `kind: Deploy` на `kind: Deployment`
</b></details>

<details>
<summary>Исправьте следующий манифест развертывания

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: dep
  name: dep
spec:
  replicas: 3
  selector:
    matchLabels:
      app: depdep
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: dep
    spec:
      containers:
      - image: redis
        name: redis
        resources: {}
status: {}
```
</summary><br><b>

Селектор не соответствует метке (dep против depdep). Чтобы решить эту проблему, исправьте depdep на dep.
</b></details>

#### Команды Развертываний

<details>
<summary>Создайте файл определения/манифеста развертывания с именем "dep", с 3 репликами, который использует изображение 'redis'</summary><br><b>

`k create deploy dep -o yaml --image=redis --dry-run=client --replicas 3 > deployment.yaml `

</b></details>

<details>
<summary>Удалить развертывание `depdep`</summary><br><b>

`k delete deploy depdep`

</b></details>

<details>
<summary>Создайте развертывание с именем "pluck", используя изображение "redis", и убедитесь, что оно запущено с 5 репликами</summary><br><b>

`kubectl create deployment pluck --image=redis`

`kubectl scale deployment pluck --replicas=5`
</b></details>

<details>
<summary>Создайте развертывание с следующими свойствами:

* называется "blufer"
* использует изображение "python"
* запускает 3 реплики
* все pods будут размещены на узле, который имеет метку "blufer"
</summary><br><b>

`kubectl create deployment blufer --image=python --replicas=3 -o yaml --dry-run=client > deployment.yaml`

Добавьте следующий раздел (`vi deployment.yaml`):

```
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: blufer
            operator: Exists
```

`kubectl apply -f deployment.yaml`
</b></details>

### Сервисы

<details>
<summary>Что такое Сервис в Kubernetes?</summary><br><b>

"Абстрактный способ выставить приложение, работает на наборе Pods, как сетевой сервис." - читайте больше [здесь](https://kubernetes.io/docs/concepts/services-networking/service)

Простыми словами, это позволяет вам добавить внутреннюю или внешнюю связь к определенному приложению, работающему в контейнере.
</b></details>

<details>
<summary>Поместите компоненты в правильные заполнители в отношении сервиса Kubernetes<br>
<img src="images/service_exercise.png"/>
</summary><br><b>

<img src="images/service_solution.png"/>

</b></details>

<details>
<summary>Как создать сервис для существующего развертывания с именем "alle" на порту 8080, чтобы Pod(ы) были доступны через балансировщик нагрузки?</summary><br><b>

Императивным способом:

`kubectl expose deployment alle --type=LoadBalancer --port 8080`
</b></details>

<details>
<summary>Истина или Ложь? Жизненный цикл Pods и Сервисов не связан, так что когда Pod умирает, Сервис по-прежнему остаётся</summary><br><b>

Истина
</b></details>

<details>
<summary>После создания сервиса, как проверить, что он был создан?</summary><br><b>

`kubectl get svc`
</b></details>

<details>
<summary>Какой тип сервиса по умолчанию?</summary><br><b>

ClusterIP - используется для внутренней связи.
</b></details>

<details>
<summary>Какие существуют типы служб?</summary><br><b>

* ClusterIP
* NodePort
* LoadBalancer
* ExternalName

Подробнее по этой теме [здесь](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)
</b></details>

<details>
<summary>Как Сервис и Развертывание связаны?</summary><br><b>

Правда в том, что они не связаны. Сервис указывает непосредственно на Pod(ы), не соединяясь с Развертыванием никаким образом.
</b></details>

<details>
<summary>Какие важные шаги в определении/добавлении Сервиса?</summary><br><b>

1. Убедитесь, что targetPort Сервиса совпадает с containerPort Pod
2. Убедитесь, что селектор соответствует хотя бы одной из меток Pod

</b></details>

<details>
<summary>Какой тип сервиса по умолчанию в Kubernetes и для чего он используется?</summary><br><b>

По умолчанию это ClusterIP, и он используется для выставления порта внутренне. Это полезно, когда вы хотите обеспечить внутреннее взаимодействие между Pods и предотвратить какой-либо внешний доступ.

</b></details>

<details>
<summary>Как получить информацию по определенной службе?</summary><br><b>

`kubectl describe service <SERVICE_NAME>`

Чаще бывает использовать `kubectl describe svc ...`

</b></details>

<details>
<summary>Что делает следующая команда?

```
kubectl expose rs some-replicaset --name=replicaset-svc --target-port=2017 --type=NodePort
```
</summary><br><b>

Это открывает ReplicaSet, создавая службу под названием 'replicaset-svc'. Открываемый порт - 2017 (это порт, используемый приложением), а тип службы - NodePort, что означает, что он будет доступен извне.
</b></details>

<details>
<summary>Истина или Ложь? целевой порт, в случае выполнения следующей команды, будет открыт только на одном из узлов кластера Kubernetes, но будет перенаправлен на все Pods

```
kubectl expose rs some-replicaset --name=replicaset-svc --target-port=2017 --type=NodePort
```
</summary><br><b>

Ложь. Он будет открыт на каждом узле кластера и будет перенаправлен на один из Pods (которые принадлежат ReplicaSet).
</b></details>

<details>
<summary>Как подтвердить, что конкретная служба настроена для перенаправления запросов на данный pod</summary><br><b>

Выполните `kubectl describe service` и проверьте, совпадают ли IP-адреса из "Endpoints" с любыми IP-адресами из вывода `kubectl get pod -o wide`.
</b></details>

<details>
<summary>Объясните, что произойдет при выполнении действия на следующем блоке

```
apiVersion: v1
kind: Service
metadata:
  name: some-app
spec:
  type: NodePort
  ports:
  - port: 8080
    nodePort: 2017
    protocol: TCP
  selector:
    type: backend
    service: some-app
```
</summary><br><b>

Это создаст новый Сервис типа "NodePort", который может быть использован для внутренней и внешней связи с приложением.<br>
Порт приложения - это 8080, и запросы будут перенаправлены на этот порт. Открываемый порт - это 2017. Замечание: Это не распространенная практика - указывать nodePort.<br>
Порт использует TCP (вместо UDP), и это также значение по умолчанию, поэтому вы не обязаны его указывать.<br>
Селектор, используемый Сервисом, чтобы знать, на какие Pods отправлять запросы. В данном случае Pods имеют метку "type: backend" и "service: some-app".<br>
</b></details>

<details>
<summary>Как превратить следующий сервис во внешний?

```
spec:
  selector:
    app: some-app
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
```
</summary><br><b>

Добавить `type: LoadBalancer` и `nodePort`

```
spec:
  selector:
    app: some-app
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
      nodePort: 32412
```
</b></details>

<details>
<summary>Как бы вы направили трафик от внешнего адреса к вашему приложению, работающему в Kubernetes?</summary><br><b>

Используя Ingress.
</b></details>

<details>
<summary>Истина или Ложь? Когда используется "NodePort", "ClusterIP" автоматически создается?</summary><br><b>

Истина
</b></details>

<details>
<summary>Когда же вы будете использовать тип "LoadBalancer"?</summary><br><b>

В основном, когда вы хотите соединить его с балансировщиком нагрузки от провайдера облака.
</b></details>

<details>
<summary>Как бы вы сопоставили службу внешнему адресу?</summary><br><b>

Используя директиву 'ExternalName'.
</b></details>

<details>
<summary>Подробно опишите, что происходит, когда вы создаете службу</summary><br><b>

1. Kubectl отправляет запрос на сервер API для создания Службы
2. Контроллер обнаруживает, что есть новая Служба
3. Объекты EndPoint создаются с тем же именем, что и служба, контроллером
4. Контроллер использует селектор Службы, чтобы определить точки окончания
5. kube-proxy обнаруживает, что есть новый объект окончания + новая служба и добавляет правила iptables для захвата трафика на порт Службы и перенаправления к точкам окончания
6. kube-dns обнаруживает, что есть новая Служба и добавляет контейнерную запись на DNS-сервер
</b></details>

<details>
<summary>Как перечислить конечные точки определенного приложения?</summary><br><b>

`kubectl get ep <name>`
</b></details>

<details>
<summary>Как можно узнать информацию о Службе, относящейся к определенному Pod, если все, что вы можете использовать, это <code>kubectl exec <POD_NAME> -- </code></summary><br><b>

Вы можете выполнить `kubectl exec <POD_NAME> -- env`, что даст вам несколько переменных окружения, связанных с Службой.<br>
Переменные такие как `[SERVICE_NAME]_SERVICE_HOST`, `[SERVICE_NAME]_SERVICE_PORT`, ...
</b></details>

<details>
<summary>Опишите, что происходит, когда контейнер впервые пытается подключиться к своей соответствующей службе. Объясните, кто добавил каждый из компонентов, которые Вы включаете в свое описание</summary><br><b>

  - Контейнер обращается к серверу имен, определенному в /etc/resolv.conf
  - Контейнер делает запрос на сервер имен, чтобы адрес был разрешен заблуждению 
  - Запросы к IP-адресу Службы перенаправляются с помощью правил iptables (или другого выбранного программного обеспечения) на конечные точки.

Объяснение, кто добавил их:

  - Сервер имен внутри контейнера добавлен kubelet во время планирования Pod, используя kube-dns
  - DNS-запись службы добавляется kube-dns во время создания Службы
  - Правила iptables добавляются kube-proxy во время создания конечной точки и службы
</b></details>

<details>
<summary>Опишите в общих чертах, что происходит, когда вы выполняете <code>kubectl expose deployment remo --type=LoadBalancer --port 8080</code></summary><br><b>

1. Kubectl отправляет запрос на API Kubernetes для создания объекта Службы
2. Kubernetes обращается к провайдеру облака (например, AWS, GCP, Azure) для получения балансировщика нагрузки
3. Новый балансировщик нагрузки перенаправляет входящий трафик на соответствующие узлы рабочих узлов, которые затем перенаправляют трафик на соответствующие контейнеры
</b></details>

<details>
<summary>После создания службы, которая перенаправляет входящий внешний трафик на контейнеризированное приложение, как убедиться, что это работает?</summary><br><b>

Вы можете выполнить `curl <SERVICE IP>:<SERVICE PORT>`, чтобы проверить вывод.
</b></details>

<details>
<summary>Внутренний балансировщик нагрузки в Kubernetes называется <code>____</code>, а внешний балансировщик нагрузки называется <code>____</code></summary><br><b>

Внутренний балансировщик нагрузки в Kubernetes называется Сервис, а внешний балансировщик нагрузки - Ingress.
</b></details>

### Ingress

<details>
<summary>Что такое Ingress?</summary><br><b>

Согласно документации Kubernetes: "Ingress открывает HTTP и HTTPS маршруты снаружи кластера к службам внутри кластера. Маршрутизация трафика контролируется правилами, определенными в ресурсе Ingress."

Читайте подробнее [здесь](https://kubernetes.io/docs/concepts/services-networking/ingress/)
</b></details>

<details>
<summary>Дополните следующий конфигурационный файл, чтобы сделать его Ingress

```
metadata:
  name: someapp-ingress
spec:
```
</summary><br><b>
Существует несколько способов ответить на этот вопрос.

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: someapp-ingress
spec:
  rules:
  - host: my.host
    http:
      paths:
      - backend:
          serviceName: someapp-internal-service
          servicePort: 8080
```
</b></details>

<details>
<summary>Объясните значение "http", "host" и "backend" директив

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: someapp-ingress
spec:
  rules:
  - host: my.host
    http:
      paths:
      - backend:
          serviceName: someapp-internal-service
          servicePort: 8080
```
</summary><br><b>

host - это входная точка кластера, так что это фактически действительный доменное имя, которое сопоставляется с IP-адресом узла кластера.<br>
строка http используется для указания того, что входящие запросы будут перенаправлены на внутреннюю службу с использованием http.<br>
backend ссылается на внутреннюю службу (serviceName - это имя в метаданных, а servicePort - это порт в разделе портов).
</b></details>

<details>
<summary>Почему использование подстановочного знака в хосте ingress может привести к проблемам?</summary><br><b>

Причина, по которой вы не должны использовать значение подстановочного знака в хосте (например, `- host: *`), заключается в том, что вы, по сути, говорите вашему кластеру Kubernetes перенаправлять весь трафик на контейнер, для которого используется этот ingress. Это может привести к сбоям всего кластера.
</b></details>

<details>
<summary>Что такое контроллер Ingress?</summary><br><b>

Реализация Ingress. Это, по сути, еще один pod (или набор pod), который оценивает и обрабатывает правила Ingress и управляет всеми перенаправлениями. 

Существует несколько реализаций контроллера Ingress (одна из них - это Nginx Ingress Controller от Kubernetes).
</b></details>

<details>
<summary>Какие есть случаи использования для использования Ingress?</summary><br><b>

* Несколько поддоменов (несколько записей хоста, каждая из которых имеет свою службу)
* Один домен с несколькими службами (несколько путей, где каждый график сопоставлен с различной службой/приложением)
</b></details>

<details>
<summary>Как перечислить Ingress в вашем пространстве имен?</summary><br><b>

kubectl get ingress
</b></details>

<details>
<summary>Что такое Default Backend Ingress?</summary><br><b>

Это указывает, что делать с входящим запросом к кластеру Kubernetes, который не сопоставлен ни с одним бэкендом (т.е. нет правила для сопоставления запроса с службой). Если служба по умолчанию не определена, рекомендуется определить ее так, чтобы пользователи все равно видели какое-то сообщение, а не ничего или неопределенную ошибку.
</b></details>

<details>
<summary>Как настроить бэкенд по умолчанию?</summary><br><b>

Создайте объект Службы, который указывает имя бэкенда по умолчанию, как отражено в `kubectl describe ingress ...` и порт в разделе портов.
</b></details>

<details>
<summary>Как настроить TLS с Ingress?</summary><br><b>

Добавить tls и secretName записи.

```
spec:
  tls:
  - hosts:
    - some_app.com
    secretName: someapp-secret-tls
```
</b></details>

<details>
<summary>Истина или Ложь? При настройке Ingress с использованием TLS, компонент Secret должен находиться в том же пространстве имен, что и компонент Ingress</summary><br><b>

Истина
</b></details>

<details>
<summary>Какую концепцию Kubernetes вы бы использовали для контроля потока трафика на уровне IP-адреса или порта?</summary><br><b>

Сетевые Политики
</b></details>

<details>
<summary>Как масштабировать приложение (развертывание), чтобы оно работало более чем в одном экземпляре приложения?</summary><br><b>

Чтобы запустить два экземпляра приложения?

`kubectl scale deployment <DEPLOYMENT_NAME> --replicas=2`

Вы можете указать любое другое число, учитывая, что ваше приложение знает, как масштабироваться.
</b></details>

### ReplicaSets

<details>
<summary>Какова цель ReplicaSet?</summary><br><b>

[kubernetes.io](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset): "Цель ReplicaSet - поддерживать стабильный набор репликантных Pods, работающих в любое время. Таким образом, он часто используется для обеспечения доступности заданного числа идентичных Pods."

Проще говоря, ReplicaSet обеспечит, чтобы заданное число реплик Pods работало для выбранного Pod. Если Pods больше, чем определено в ReplicaSet, некоторые будут удалены. Если Pods меньше, чем определено в ReplicaSet, то больше реплик будет добавлено.
</b></details>

<details>
<summary>Что делает следующий блок строк?

```
spec:
  replicas: 2
  selector:
    matchLabels:
      type: backend
  template:
    metadata:
      labels:
        type: backend
    spec:
      containers:
      - name: httpd-yup
        image: httpd
```
</summary><br><b>

Это определяет replicaset для Pods, метка которых установлена в "backend", так что в любой момент времени будет работать 2 параллельных Pod.
</b></details>

<details>
<summary>Что произойдет, когда Pod, созданный ReplicaSet, будет удален непосредственно с помощью <code>kubectl delete po ...</code>?</summary><br><b>

ReplicaSet создаст новый Pod, чтобы достичь желаемого количества реплик.
</b></details>

<details>
<summary>Истина или Ложь? Если ReplicaSet определяет 2 реплики, но 3 Pods, соответствующие селектору ReplicaSet, работают, он ничего не сделает</summary><br><b>

Ложь. Он завершит один из Pods, чтобы достичь желаемого состояния из 2 реплик.
</b></details>

<details>
<summary>Опишите последовательность событий в случае создания ReplicaSet</summary><br><b>

* Клиент (например, kubectl) отправляет запрос на сервер API для создания ReplicaSet
* Контроллер обнаруживает, что есть новое событие, запрашивающее ReplicaSet
* Контроллер создает новые определения Pods (точное количество зависит от того, что определено в определении ReplicaSet)
* Планировщик обнаруживает неподписанные Pods и решает, к каким узлам их назначить. Эта информация отправляется на сервер API
* Kubelet обнаруживает, что два Pods были назначены узлу, на котором он запускается (так как он постоянно следит за сервером API)
* Kubelet отправляет запрос на движок контейнера для создания контейнеров, входящих в Pod
* Kubelet отправляет запрос на сервер API, чтобы уведомить его о создании Pods
</b></details>

<details>
<summary>Как перечислить ReplicaSets в текущем пространстве имен?</summary><br><b>

`kubectl get rs`
</b></details>

<details>
<summary>Можно ли удалить ReplicaSet без удаления Pods, которые он создал?</summary><br><b>

Да, с `--cascade=false`.

`kubectl delete -f rs.yaml --cascade=false`
</b></details>

<details>
<summary>Какое число реплик по умолчанию, если оно не указано явно?</summary><br><b>

1
</b></details>

<details>
<summary>Что означает следующий вывод <code>kubectl get rs</code>?

NAME                    DESIRED   CURRENT   READY   AGE
web                     2         2         0       2m23s
</summary><br><b>

ReplicaSet `web` имеет 2 реплики. Похоже, что контейнеры внутри Pod(ов) еще не работают, поскольку значение READY равно 0. Это может быть нормальным, так как некоторым контейнерам требуется время для запуска, и это может быть связано с ошибкой. Выполнение `kubectl describe po POD_NAME` или `kubectl logs POD_NAME` может дать дополнительную информацию.
</b></details>
<details>
<summary>Верно или Ложно? Поды, указанные в поле селектора ReplicaSet, должны быть созданы самим ReplicaSet</summary><br><b>

Ложно. Поды могут уже работать и изначально могут быть созданы любым объектом. Это не имеет значения для ReplicaSet и не является требованием для его получения и мониторинга.
</b></details>

<details>
<summary>Верно или Ложно? В случае ReplicaSet, если Поды, указанные в поле селектора, не существуют, ReplicaSet будет ждать, пока они не запустятся, прежде чем что-либо делать</summary><br><b>

Ложно. Он позаботится о запуске отсутствующих Подов.
</b></details>

<details>
<summary>В случае ReplicaSet, какое поле обязательно в разделе spec?</summary><br><b>

Поле `template` в разделе spec обязательно. Оно используется ReplicaSet для создания новых Подов при необходимости.
</b></details>

<details>
<summary>Вы создали ReplicaSet, как проверить, нашёл ли ReplicaSet подходящие Поды или создал новые?</summary><br><b>

`kubectl describe rs <ReplicaSet Name>`

Это будет видно в разделе `Events` (последние строки)
</b></details>

<details>
<summary>Верно или Ложно? Удаление ReplicaSet удалит Подов, которые он создал</summary><br><b>

Верно (и не только Подов, но и всего остального, что он создал).
</b></details>

<details>
<summary>Верно или Ложно? Удаление метки из Пода, который отслеживается ReplicaSet, приведет к созданию нового Подом из ReplicaSet</summary><br><b>

Верно. Когда метка, используемая ReplicaSet в поле селектора, удаляется из Под, этот Под больше не контролируется ReplicaSet, и ReplicaSet создаст новый Под в компенсацию за тот, который он "потерял".
</b></details>

<details>
<summary>Как масштабировать развертывание до 8 реплик?</summary><br><b>

kubectl scale deploy <DEPLOYMENT_NAME> --replicas=8
</b></details>

<details>
<summary>ReplicaSets работают в момент, когда пользователь выполняет команду на их создание (например, <code>kubectl create -f rs.yaml</code>)</summary><br><b>

Ложно. Это может занять некоторое время в зависимости от того, что именно вы запускаете. Чтобы увидеть, работают ли они, выполните команду `kubectl get rs` и посмотрите на столбец 'READY'.
</b></details>

<details>
<summary>Как выставить ReplicaSet как новый сервис?</summary><br><b>

`kubectl expose rs <ReplicaSet Name> --name=<Service Name> --target-port=<Port to expose> --type=NodePort`

Несколько примечаний:
  - целевой порт зависит от того, какой порт использует приложение в контейнере
  - тип может быть различным и не обязательно должен быть "NodePort"
</b></details>

<details>
<summary>Исправьте следующее определение ReplicaSet

```yaml
apiVersion: apps/v1
kind: ReplicaCet
metadata:
  name: redis
  labels:
    app: redis
    tier: cache
spec:
  selector:
    matchLabels:
      tier: cache
  template:
    metadata:
      labels:
        tier: cachy
    spec:
      containers:
      - name: redis
        image: redis
```
</summary><br><b>

kind должен быть ReplicaSet, а не ReplicaCet :)
</b></details>

<details>
<summary>Исправьте следующее определение ReplicaSet

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: redis
  labels:
    app: redis
    tier: cache
spec:
  selector:
    matchLabels:
      tier: cache
  template:
    metadata:
      labels:
        tier: cachy
    spec:
      containers:
      - name: redis
        image: redis
```
</summary><br><b>

Селектор не совпадает с меткой (cache vs cachy). Чтобы решить это, исправьте cachy так, чтобы она стала cache.
</b></details>

<details>
<summary>Как проверить, какой контейнерный образ использовался в ReplicaSet под названием "repli"?</summary><br><b>

`k describe rs repli | grep -i image`
</b></details>

<details>
<summary>Как проверить, сколько Подов готовы в ReplicaSet под названием "repli"?</summary><br><b>

`k describe rs repli | grep -i "Pods Status"`
</b></details>

<details>
<summary>Как удалить ReplicaSet под названием "rori"?</summary><br><b>

`k delete rs rori`
</b></details>

<details>
<summary>Как изменить ReplicaSet под названием "rori", чтобы использовать другой образ?</summary><br><b>

`k edit rs rori`
</b></details>

<details>
<summary>Увеличьте ReplicaSet под названием "rori" до 5 Подов вместо 2</summary><br><b>

`k scale rs rori --replicas=5`
</b></details>

<details>
<summary>Снизьте ReplicaSet под названием "rori" до 1 Пода вместо 5</summary><br><b>

`k scale rs rori --replicas=1`
</b></details>

### DaemonSet

<details>
<summary>Что такое DaemonSet?</summary><br><b>

[Kubernetes.io](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset): "DaemonSet гарантирует, что все (или некоторые) узлы запускают копию Пода. По мере добавления узлов в кластер, Подов добавляется к ним. По мере удаления узлов из кластера, эти Поды собираются мусорщиком. Удаление DaemonSet очистит Поды, которые он создал."
</b></details>

<details>
<summary>В чем отличие между ReplicaSet и DaemonSet?</summary><br><b>

Цель ReplicaSet заключается в поддержании стабильного набора реплицируемых Подов, работающих в любой момент времени.
DaemonSet гарантирует, что все Узлы запускают копию Пода.
</b></details>

<details>
<summary>Какие случаи использования DaemonSet?</summary><br><b>

* Мониторинг: вам нужно провести мониторинг на каждом узле, входящем в кластер. Например, Pod datadog работает на каждом узле, используя DaemonSet.
* Логирование: вам нужно настроить логирование на каждом узле, входящем в ваш кластер.
* Сетевые компоненты: вам нужен сетевой компонент на каждом узле, чтобы все узлы могли общаться друг с другом.
</b></details>

<details>
<summary>Как работает DaemonSet?</summary><br><b>

Исторически, до версии 1.12, это осуществлялось с помощью атрибута NodeName.

Начиная с 1.12, это достигается с помощью обычного планировщика и аффинности узлов.
</b></details>

#### DaemonSet - Команды

<details>
<summary>Как перечислить все DaemonSet в текущем пространстве имен?</summary><br><b>

`kubectl get ds`
</b></details>

### StatefulSet

<details>
<summary>Объясните StatefulSet</summary><br><b>

StatefulSet — это объект API нагрузки, используемый для управления состоянием приложений. Управляет развертыванием и масштабированием набора Подов и обеспечивает гарантии относительно порядка и уникальности этих Подов.[Узнать больше](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)
</b></details>

### Хранение

#### Тома

<details>
<summary>Что такое том в отношении Kubernetes?</summary><br><b>

Директория, доступная контейнерам внутри определенного Пода и контейнеров. Механизм, ответственный за создание директории, управление ей и т. д., в основном зависит от типа тома.
</b></details>

<details>
<summary>С какими типами томов вы знакомы?</summary><br><b>

* emptyDir: создается, когда Под назначается на узел, и прекращает существовать, когда Под больше не работает на этом узле.
* hostPath: монтирует путь с самого хоста. Обычно не используется из-за рисков безопасности, но имеет множество случаев использования, где это необходимо, например, доступ к некоторым внутренним путям хоста (`/sys`, `/var/lib` и т.д.)
</b></details>

<details>
<summary>Какие проблемы решают тома в Kubernetes?</summary><br><b>

1. Совместное использование файлов между контейнерами, работающими в одном Поде.
2. Хранение в контейнерах является эфемерным — оно обычно не длится долго. Например, когда контейнер завершается, вы теряете все данные на диске. Некоторые тома позволяют управлять такой ситуацией с помощью постоянных томов.

</b></details>

<details>
<summary>Объясните эфемерные типы томов против постоянных томов в отношении Подов</summary><br><b>

Эфемерные типы томов существуют в течение всего времени жизни Пода, в отличие от постоянных томов, которые существуют за пределами времени жизни Пода.
</b></details>

<details>
<summary>Приведите хотя бы один случай использования для каждого из следующих типов томов:

* emptyDir
* hostPath
</summary><br><b>

* EmptyDir: вам нужны временные данные, которые вы можете позволить себе потерять, если Под будет удален. Например, недолговечные данные, необходимые для одноразовых операций.
* hostPath: вам нужен доступ к путям на самом хосте (например, данные из `/sys` или данные, сгенерированные в `/var/lib`).
</b></details>

### Сеть

<details>
<summary>Верно или Ложно? По умолчанию между двумя Подами в двух разных пространствах имен нет связи</summary><br><b>

Ложно. По умолчанию два Под в двух разных пространствах имен могут общаться друг с другом.

Попробуйте сами:

kubectl run test-prod -n prod --image ubuntu -- sleep 2000000000
kubectl run test-dev -n dev --image ubuntu -- sleep 2000000000

`k describe po test-prod -n prod`, чтобы получить IP Пода test-prod.

Доступ к Pod dev: `kubectl exec --stdin --tty test-dev -n dev -- /bin/bash`

И пинг IP Пода test-prod, который вы получили ранее. Вы увидите, что существует связь между двумя Подами в двух отдельных пространствах имен.
</b></details>

### Сетевые политики

<details>
<summary>Объясните сетевые политики</summary><br><b>

[kubernetes.io](https://kubernetes.io/docs/concepts/services-networking/network-policies): "Сетевые политики — это конструкция, ориентированная на приложение, которая позволяет вам указать, как Поду разрешено взаимодействовать с различными сетевыми 'сущностями'..."

Проще говоря, сетевые политики указывают, как поды могут или не могут взаимодействовать друг с другом и/или другими сетевыми конечными точками.
</b></details>

<details>
<summary>Каковы некоторые случаи использования сетевых политик?</summary><br><b>

  - Безопасность: вы хотите предотвратить доступ всех к определенному Поду по причинам безопасности.
  - Управление сетевым трафиком: вы хотите запретить сетевой поток между двумя конкретными узлами.
</b></details>

<details>
<summary>Верно или Ложно? Если к поду не применяются сетевые политики, то разрешены соединения к нему и от него</summary><br><b>

Ложно. По умолчанию поды не изолированы.
</b></details>

<details>
<summary>В случае двух подов, если на источнике есть полиса egress, определяющая трафик, и на пункте назначения есть полиса ingress, позволяющая трафик, то трафик будет разрешен или отклонен?</summary><br><b>

Отклонен. Полисы и источника, и назначения должны разрешать трафик для его разрешения.
</b></details>

<details>
<summary>Где Kubernetes-кластер хранит состояние кластера?</summary><br><b>

etcd
</b></details>

### etcd

<details>
<summary>Что такое etcd?</summary><br><b>

etcd — это хранилище ключ-значение с открытым исходным кодом, используемое для хранения и управления критической информацией, необходимой распределенным системам для поддержания работы.

[Узнать больше здесь](https://www.redhat.com/en/topics/containers/what-is-etcd)
</b></details>

<details>
<summary>Верно или Ложно? Etcd хранит текущее состояние любого компонента Kubernetes</summary><br><b>

Верно
</b></details>

<details>
<summary>Верно или Ложно? API сервер — единственный компонент, который напрямую общается с etcd</summary><br><b>

Верно
</b></details>

<details>
<summary>Верно или Ложно? Данные приложения не хранятся в etcd</summary><br><b>

Верно
</b></details>

<details>
<summary>Почему etcd? Почему не какая-либо SQL или NoSQL база данных?</summary><br><b>

Когда etcd был выбран в качестве хранилища данных, он был (и, конечно же, остается):

* Высоко доступным — вы можете развернуть несколько узлов.
* Полностью реплицируемым — любой узел в кластере etcd является "основным" узлом и имеет полный доступ к данным.
* Консистентным — чтения возвращают последние данные.
* Безопасным — поддерживает как TLS, так и SSL.
* Быстрым — высокопроизводительное хранилище данных (10k записей в секунду!)
</b></details>

### Пространства имен

<details>
<summary>Что такое пространства имен?</summary><br><b>

Пространства имен позволяют вам разделить ваш кластер на виртуальные кластеры, в которых вы можете группировать ваши приложения таким образом, который имеет смысл и полностью отделен от других групп (например, вы можете создать приложение с тем же именем в двух разных пространствах имен).
</b></details>

<details>
<summary>Зачем использовать пространства имен? В чем проблема с использованием одного пространства имен по умолчанию?</summary><br><b>

При использовании только пространства имен по умолчанию со временем становится трудно получить обзор всех приложений, которые вы управляете в вашем кластере. Пространства имен упрощают организацию приложений в группы, которые имеют смысл, такие как пространство имен всех приложений мониторинга и пространство имен для всех приложений безопасности и т.д.

Пространства имен также могут быть полезны для управления средами Blue/Green, где каждое пространство имен может включать различную версию приложения и также делиться ресурсами, которые находятся в других пространствах имен (например, такие как логирование, мониторинг и т.д.).

Другой случай использования пространств имен — это один кластер для нескольких команд. Когда несколько команд используют один и тот же кластер, они могут в конечном итоге перетаскиваться друг с другом. Например, если они в конечном итоге создают приложение с одним и тем же именем, это означает, что одна из команд перезаписала приложение другой команды, потому что в Kubernetes не может быть двух приложений с одним и тем же именем (в одном и том же пространстве имен).
</b></details>

<details>
<summary>Верно или Ложно? Когда пространство имен удаляется, все ресурсы в этом пространстве имен не удаляются, а перемещаются в другое пространство имен по умолчанию</summary><br><b>

Ложно. Когда пространство имен удаляется, ресурсы в этом пространстве имен также удаляются.
</b></details>

<details>
<summary>Какие специальные пространства имен существуют по умолчанию при создании кластера Kubernetes?</summary><br><b>

* default
* kube-system
* kube-public
* kube-node-lease
</b></details>

<details>
<summary>Что можно найти в пространстве имен kube-system?</summary><br><b>

* Процессы Master и Kubectl
* Системные процессы
</b></details>

<details>
<summary>Хотя пространства имен действительно предоставляют область действия для ресурсов, они не изолируют их</summary><br><b>

Верно. Попробуйте создать два пода в двух отдельных пространствах имен, и вы увидите, что между ними есть связь.
</b></details>

#### Пространства имен - команды

<details>
<summary>Как перечислить все пространства имен?</summary><br><b>

`kubectl get namespaces` ИЛИ `kubectl get ns`

</b></details>

<details>
<summary>Создайте пространство имен с названием 'alle'</summary><br><b>

`k create ns alle`

</b></details>

<details>
<summary>Узнайте, сколько пространств имен существует</summary><br><b>

`k get ns --no-headers | wc -l`

</b></details>

<details>
<summary>Проверить, сколько подов существует в пространстве имен "dev"</summary><br><b>

`k get po -n dev`

</b></details>

<details>
<summary>Создайте под с названием "kartos" в пространстве имен dev. Под должен использовать образ "redis".</summary><br><b>

Если пространство имен еще не существует: `k create ns dev`

`k run kratos --image=redis -n dev`

</b></details>

<details>
<summary>Вы ищете Под с названием "atreus". Как проверить, в каком пространстве имен он работает?</summary><br><b>

`k get po -A | grep atreus`
</b></details>

<details>
<summary>Что содержит kube-public?</summary><br><b>

* ConfigMap, который содержит информацию о кластере
* Общедоступные данные
</b></details>

<details>
<summary>Как получить название текущего пространства имен?</summary><br><b>

`kubectl config view | grep namespace`
</b></details>

<details>
<summary>Что содержит kube-node-lease?</summary><br><b>

Он хранит информацию о сигналах жизнедеятельности узлов. Каждый узел получает объект, который хранит информацию о его доступности.
</b></details>

<details>
<summary>Верно или Ложно? С помощью пространств имен вы можете ограничить потребляемые ресурсы пользователями/командами</summary><br><b>

Верно. С помощью пространств имен вы можете ограничить использование процессора, оперативной памяти и хранилища.
</b></details>

<details>
<summary>Как переключиться на другое пространство имен? Другими словами, как изменить активное пространство имен?</summary><br><b>

`kubectl config set-context --current --namespace=some-namespace` и проверьте с помощью `kubectl config view --minify | grep namespace:`

ИЛИ

`kubens some-namespace`
</b></details>

#### Квота ресурсов

<details>
<summary>Что такое квота ресурсов?</summary><br><b>

Квота ресурсов предоставляет ограничения, которые ограничивают совокупное потребление ресурсов по пространству имен. Она может ограничивать количество объектов, которые могут быть созданы в пространстве имен по типу, а также общее количество вычислительных ресурсов, которые могут быть потреблены ресурсами в этом пространстве имен.
</b></details>

<details>
<summary>Как создать квоту ресурсов?</summary><br><b>

kubectl create quota some-quota --hard-cpu=2,pods=2
</b></details>

<details>
<summary>Какие ресурсы доступны из разных пространств имен?</summary><br><b>

Сервисы.
</b></details>

<details>
<summary>На какой сервис и в каком пространстве имен ссылается следующий файл?

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: some-configmap
data:
  some_url: samurai.jack
```
</summary><br><b>

Он ссылается на сервис "samurai" в пространстве имен "jack".
</b></details>

<details>
<summary>Какие компоненты не могут быть созданы в пространстве имен?</summary><br><b>

Том и Узел.
</b></details>

<details>
<summary>Как перечислить все компоненты, связанные с пространством имен?</summary><br><b>

`kubectl api-resources --namespaced=true`
</b></details>

<details>
<summary>Как создать компоненты в пространстве имен?</summary><br><b>

Один из способов — указать --namespace, вот так: `kubectl apply -f my_component.yaml --namespace=some-namespace`
Другой способ — указать это в YAML самом:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: some-configmap
  namespace: some-namespace
```

И вы можете проверить это с помощью: `kubectl get configmap -n some-namespace`
</b></details>

<details>
<summary>Как выполнить команду "ls" в существующем поде?</summary><br><b>

kubectl exec some-pod -it -- ls
</b></details>

<details>
<summary>Как создать сервис, который выставит развертывание?</summary><br><b>

kubectl expose deploy some-deployment --port=80 --target-port=8080
</b></details>

<details>
<summary>Как создать под и сервис одной командой?</summary><br><b>

kubectl run nginx --image=nginx --restart=Never --port 80 --expose
</b></details>

<details>
<summary>Подробно опишите, что делает следующая команда <code>kubectl create deployment kubernetes-httpd --image=httpd</code></summary><br><b>
</b></details>

<details>
<summary>Зачем создавать развертывание, если поды можно запускать с помощью ReplicaSet?</summary><br><b>
</b></details>

<details>
<summary>Как получить список ресурсов, которые не привязаны к конкретному пространству имен?</summary><br><b>

kubectl api-resources --namespaced=false
</b></details>

<details>
<summary>Как удалить все поды, статус которых не "Running"?</summary><br><b>

kubectl delete pods --field-selector=status.phase!='Running'
</b></details>

<details>
<summary>Как отобразить использование ресурсов подов?</summary><br><b>

kubectl top pod
</b></details>

<details>
<summary>Возможно, это общий вопрос, но вы подозреваете, что один из подов вызывает проблемы, и вы не знаете, что именно. Что вы делаете?</summary><br><b>

Начните с проверки статуса подов. Мы можем использовать команду `kubectl get pods` (--all-namespaces для подов в системном пространстве имен)<br>

Если мы видим статус "Error", мы можем продолжить отладку, выполнив команду `kubectl describe pod [name]`. В случае, если мы все еще не увидим ничего полезного, мы можем попробовать stern для отслеживания журналов.<br>

В случае, если мы обнаружим, что с подом или системой была временная проблема, мы можем попробовать перезапустить под с помощью следующей команды: `kubectl scale deployment [name] --replicas=0`<br>

Установка реплик на 0 остановит процесс. Теперь запустите его с помощью `kubectl scale deployment [name] --replicas=1`.
</b></details>

<details>
<summary>Что произойдет, если поды используют слишком много памяти? (больше их лимита)</summary><br><b>

Они становятся кандидатами на завершение.
</b></details>

<details>
<summary>Опишите, как работает откат</summary><br><b>
</b></details>

<details>
<summary>Верно или Ложно? Память является сжимаемым ресурсом, что означает, что когда контейнер достигает лимита памяти, он продолжит работать</summary><br><b>

Ложно. CPU является сжимаемым ресурсом, в то время как память является несжимаемым ресурсом — как только контейнер достигает лимита памяти, он будет завершен.
</b></details>

### Операторы

<details>
<summary>Что такое Оператор?</summary><br><b>

Объяснено [здесь](https://kubernetes.io/docs/concepts/extend-kubernetes/operator)

"Операторы — это программные расширения для Kubernetes, которые используют настраиваемые ресурсы для управления приложениями и их компонентами. Операторы следуют принципам Kubernetes, в частности петле управления."

Проще говоря, вы можете представить себе оператор как настраиваемую петлю управления в Kubernetes.
</b></details>

<details>
<summary>Зачем нам Операторы?</summary><br><b>

Процесс управления состоянием приложений в Kubernetes не так прост, как управление статeless приложениями, где достижение желаемого состояния и обновления обрабатываются одинаково для каждой реплики. В состоянии приложений, обновление каждой реплики может потребовать разного подхода из-за природа приложения, каждая реплика может находиться в разном состоянии. Поэтому нам часто нужен человек-оператор для управления состоянием приложений. Kubernetes Operator должен помочь с этим.

Это также помогает автоматизировать стандартный процесс на нескольких кластерах Kubernetes.
</b></details>

<details>
<summary>Какие компоненты состоит Оператор?</summary><br><b>

1. CRD (Определение настраиваемого ресурса) - стандартно используются ресурсы Kubernetes, такие как Deployment, Pod, Service и т. д. CRD также является ресурсом, но тем, который вы или разработчик оператора определяет.
2. Контрольный цикл - настраиваемая петля управления, которая выполняется в противостоянии к CRD.
</b></details>

<details>
<summary>Объясните CRD</summary><br><b>

CRD — это определения пользовательских ресурсов. Это пользовательский компонент Kubernetes, который расширяет API K8s.

TODO(abregman): добавить больше информации.
</b></details>

<details>
<summary>Как работает Оператор?</summary><br><b>

Он использует контрольную петлю, используемую в Kubernetes в целом. Он наблюдает за изменениями в состоянии приложения. Разница в том, что он использует настраиваемую контрольную петлю.

Кроме того, он также использует CRD (Определения настраиваемых ресурсов), так что он, по сути, расширяет API Kubernetes.
</b></details>

<details>
<summary>Верно или Ложно? Kubernetes Operator используется для состояния приложений</summary><br><b>

Верно
</b></details>

<details>
<summary>Объясните, что такое OLM (Управление жизненным циклом оператора) и для чего оно используется</summary><br><b>

</b></details>

<details>
<summary>Что такое Операторский Фреймворк?</summary><br><b>

Открытый инструмент для управления нативными приложениями K8s, называемыми операторами, эффективным и автоматизированным образом.
</b></details>

<details>
<summary>Какие компоненты включает Операторский Фреймворк?</summary><br><b>

1. SDK Оператора - позволяет разработчикам создавать операторов.
2. Управление жизненным циклом Оператора - помогает установить, обновить и, в общем, управлять жизненным циклом всех операторов.
3. Учет Операторов - позволяет предоставлять отчеты об использовании операторов, предоставляющих специализированные услуги.
4. 
</b></details>

<details>
<summary>Подробно опишите, что такое Управление жизненным циклом Оператора</summary><br><b>

Это часть Операторского Фреймворка, используемая для управления жизненным циклом операторов. Фактически, это расширяет Kubernetes так, чтобы пользователь мог использовать декларативный способ управления операторами (установка, обновление и т. д.).
</b></details>

<details>
<summary>Что включает пространство имен openshift-operator-lifecycle-manager?</summary><br><b>

Оно включает:

  * catalog-operator - разрешение и установку ресурсов ClusterServiceVersions, которые они указывают.
  * olm-operator - Развёртывание приложений, определённых ресурсом ClusterServiceVersion.
</b></details>

<details>
<summary>Что такое kubeconfig? Для чего его используете?</summary><br><b>
  
Файл kubeconfig — это файл, используемый для настройки доступа к Kubernetes при использовании в сочетании с командным инструментом kubectl (или другими клиентами).
Используйте файлы kubeconfig для организации информации о кластерах, пользователей, пространствах имен и механизмах аутентификации.
</b></details>

<details>
<summary>Использовали бы вы Helm, Go или что-то еще для создания Оператора?</summary><br><b>
  
Это зависит от объема и зрелости Оператора. Если он в основном охватывает установку и обновления, Helm может быть достаточным. Если вы хотите заниматься управлением жизненным циклом, анализом и автопилотом, то, вероятно, вы бы использовали Go.
</b></details>

<details>
<summary>Есть ли инструменты, проекты, которые вы используете для создания Операторов?</summary><br><b>

Это основано скорее на личном опыте и вкусах...

* Операторский Фреймворк
* Kubebuilder
* Контрольный Вектор
...
</b></details>

### Секреты

<details>
<summary>Объясните Секреты Kubernetes</summary><br><b>

Секреты позволяют вам хранить и управлять конфиденциальной информацией (пароли, SSH-ключи и т. д.)
</b></details>

<details>
<summary>Как создать секрет из ключа и значения?</summary><br><b>

`kubectl create secret generic some-secret --from-literal=password='donttellmypassword'`
</b></details>

<details>
<summary>Как создать секрет из файла?</summary><br><b>

`kubectl create secret generic some-secret --from-file=/some/file.txt`
</b></details>

<details>
<summary>Что означает <code>type: Opaque</code> в файле секрета? Какие еще типы существуют?</summary><br><b>

Opaque — это тип по умолчанию, используемый для пар ключ-значение.
</b></details>

<details>
<summary>Верно или Ложно? Хранение данных в компоненте Секрета делает их автоматически безопасными</summary><br><b>

Ложно. Некоторые известные механизмы безопасности, такие как "шифрование", по умолчанию не включены.
</b></details>

<details>
<summary>В чем проблема с файлом секрета следующим образом:

```
apiVersion: v1   
kind: Secret
metadata:
    name: some-secret
type: Opaque
data:
    password: mySecretPassword
```
</summary><br><b>

Пароль не зашифрован.
Вам следует выполнить что-то подобное: `echo -n 'mySecretPassword' | base64` и вставить результат в файл вместо того, чтобы использовать простой текст.
</b></details>

<details>
<summary>Что означает следующее в файле конфигурации развертывания? 

```
spec:
  containers:
    - name: USER_PASSWORD
      valueFrom:
        secretKeyRef:
          name: some-secret
          key: password
```
</summary><br><b>

Переменная окружения USER_PASSWORD будет хранить значение из ключа password в секрете с названием "some-secret".
Другими словами, вы ссылаетесь на значение из Секрета Kubernetes.
</b></details>

<details>
<summary>Как создать резервные копии секретов в Git и вообще, как использовать зашифрованные секреты?</summary><br><b>

Одним из возможных процессов может быть следующий:

1. Вы создаете секрет Kubernetes (но не коммитите его).
2. Вы шифруете его с помощью какого-либо стороннего проекта (.e.g kubeseal).
3. Вы применяете зашифрованный секрет.
4. Вы коммитите зашифрованный секрет в Git.
5. Вы развертываете приложение, которое требует секрет, и его можно автоматически расшифровать, используя, например, контроллер зашифрованных секретов Bitnami.
</b></details>

### Томаи

<details>
<summary>Верно или Ложно? Kubernetes предоставляет постоянство данных из коробки, так что когда вы перезапускаете под, данные сохраняются</summary><br><b>

Ложно
</b></details>

<details>
<summary>Объясните "Постоянные тома". Зачем они нам нужны?</summary><br><b>

Постоянные тома позволяют нам сохранять данные, так что фактически они предоставляют хранилище, которое не зависит от жизненного цикла Пода.
</b></details>

<details>
<summary>Верно или Ложно? Постоянный том должен быть доступен на всех узлах, так как Под может перезапуститься на любом из них</summary><br><b>

Верно
</b></details>

<details>
<summary>Какие типы постоянных томов существуют?</summary><br><b>

* NFS
* iSCSI
* CephFS
* ...
</b></details>

<details>
<summary>Что такое PersistentVolumeClaim?</summary><br><b>
</b></details>

<details>
<summary>Объясните Снимки томов</summary><br><b>

Снимки томов позволяют создать копию вашего тома в определенный момент времени.
</b></details>

<details>
<summary>Верно или Ложно? Kubernetes управляет постоянством данных</summary><br><b>

Ложно
</b></details>

<details>
<summary>Объясните Классы хранилища</summary><br><b>
</b></details>

<details>
<summary>Объясните "Динамическое размещение" и "Статическое размещение"</summary><br><b>

Основное различие заключается в моменте, когда вы хотите настроить хранилище. Например, если вам нужно создать предзаполненные данные в томе, вы выбираете статическое размещение. В то время как, если вам нужно создавать тома по мере необходимости, вы выбираете динамическое размещение.
</b></details>

<details>
<summary>Объясните Режимы доступа</summary><br><b>
</b></details>

<details>
<summary>Что такое CSI Volume Cloning?</summary><br><b>
</b></details>

<details>
<summary>Объясните "Эфемерные тома"</summary><br><b>
</b></details>

<details>
<summary>С какими типами эфемерных томов поддерживает Kubernetes?</summary><br><b>
</b></details>

<details>
<summary>Что такое Политика восстановления?</summary><br><b>
</b></details>

<details>
<summary>Какие существуют политики восстановления?</summary><br><b>

* Сохранить
* Восстановить
* Удалить
</b></details>

### Управление доступом

<details>
<summary>Что такое RBAC?</summary><br><b>
 
RBAC в Kubernetes — это механизм, который позволяет вам настраивать детализированные и специфические наборы разрешений, которые определяют, как конкретный пользователь или группа пользователей могут взаимодействовать с любым объектом Kubernetes в кластере или в определённом пространстве имен кластера.
</b></details>

<details>
<summary>Объясните объекты <code>Role</code> и <code>RoleBinding</code></summary><br><b>
</b></details>

<details>
<summary>В чем разница между объектами <code>Role</code> и <code>ClusterRole</code>?</summary><br><b>
  
Разница между ними заключается в том, что Role используется на уровне пространства имен, в то время как ClusterRole - для всего кластера.
</b></details>

<details>
<summary>Объясните, что такое "Учётные записи служб" и в каком случае вы бы создали / использовали одну</summary><br><b>

[Kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account): "Учётная запись службы предоставляет идентичность для процессов, которые работают в Поде."

Пример, когда следует использовать его:
Вы определяете конвейер, который должен собирать и отправлять изображение. Чтобы иметь достаточные разрешения для сборки и отправки изображения, требуется учётная запись службы с достаточными разрешениями.
</b></details>

<details>
<summary>Что произойдет, если вы создадите под и не укажете учётную запись службы?</summary><br><b>

Под автоматически будет назначен с учётной записью службы по умолчанию (в пространстве имен, где работает под).
</b></details>

<details>
<summary>Объясните, как учётные записи службы отличаются от учётных записей пользователей</summary><br><b>

  - Учётные записи пользователей являются глобальными, в то время как учётные записи служб уникальны для каждого пространства имен.
  - Учётные записи пользователей предназначены для людей или клиентских процессов, в то время как учётные записи служб предназначены для процессов, которые работают в подах.
</b></details>

<details>
<summary>Как перечислить учётные записи служб?</summary><br><b>

`kubectl get serviceaccounts`
</b></details>

<details>
<summary>Объясните "Контекст безопасности"</summary><br><b>

[kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/security-context): "Контекст безопасности определяет настройки привилегий и контроля доступа для Пода или контейнера."
</b></details>

### Шаблоны

### CronJob

<details>
<summary>Объясните, что такое CronJob и для чего он используется</summary><br><b>
  
CronJob создает задания по повторяющемуся расписанию. Один объект CronJob - это как одна строка файла crontab (таблица cron). Он периодически выполняет задание в соответствии с заданным расписанием, записанным в формате cron.
</b></details>

<details>
<summary>Какие возможные проблемы могут возникнуть при использовании следующей спецификации и как их исправить?

```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: some-cron-job
spec:
  schedule: '*/1 * * * *'
  startingDeadlineSeconds: 10
  concurrencyPolicy: Allow
```
</summary><br><b>

Если задача cron выполняется с ошибкой, следующее задание не заменит предыдущую из-за значения "concurrencyPolicy", которое равно "Allow". Он будет продолжать создавать новые задания, и в конце концов система будет заполнена неудавшимися заданиями cron.
Чтобы избежать такой проблемы, значение "concurrencyPolicy" должно быть либо "Replace", либо "Forbid".
</b></details>

<details>
<summary>Какая проблема может возникнуть при использовании следующего CronJob и как ее исправить?

```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: "some-cron-job"
spec:
  schedule: '*/1 * * * *'
jobTemplate:
  spec:
    template:
      spec:
      restartPolicy: Never
      concurrencyPolicy: Forbid
      successfulJobsHistoryLimit: 1
      failedJobsHistoryLimit: 1
```
</summary><br><b>

Следующие строки размещены под шаблоном:

```
concurrencyPolicy: Forbid
successfulJobsHistoryLimit: 1
failedJobsHistoryLimit: 1
```

В результате эта конфигурация не является частью спецификации задания cron, следовательно, у cron job нет ограничений, что может вызвать такие проблемы, как OOM, и потенциально привести к сбою API-сервера.<br>
Чтобы исправить это, эти строки должны находиться в спецификации задания cron, выше или ниже директивы "schedule".
</b></details>

### Разное

<details>
<summary>Объясните императивное управление против декларативного управления</summary><br><b>

</b></details>

<details>
<summary>Объясните, что такое обнаружение сервисов Kubernetes</summary><br><b>
</b></details>

<details>
<summary>У вас есть один кластер Kubernetes, и несколько команд хотят его использовать. Вы хотите ограничить ресурсы, потребляемые каждой командой, в кластере. Какую концепцию Kubernetes вы бы использовали для этого?</summary><br><b>

Пространства имен позволят ограничить ресурсы и также гарантировать, что между командами не будет конфликтов, когда они работают в кластере (например, создание приложения с одним и тем же именем).
</b></details>

<details>
<summary>Что делает Kube Proxy?</summary><br><b>
  Kube Proxy — это сетевой прокси, который работает на каждом узле вашего кластера и реализует часть концепции сервиса Kubernetes.
</b></details>

<details>
<summary>Для чего используются "Квоты ресурсов" и как?</summary><br><b>
</b></details>

<details>
<summary>Объясните ConfigMap</summary><br><b>

Отделяет конфигурацию от подов.
Это хорошо в тех случаях, когда вам может потребоваться изменить конфигурацию в какой-то момент, но вы не хотите перезапускать приложение или пересобирать образ, поэтому вы создаете ConfigMap и подключаете его к поду, но внешне к поду.

В общем, это хорошо для:
* Общего использования одной и той же конфигурации между различными подами.
* Хранения конфигурации вне пода.
</b></details>

<details>
<summary>Как использовать ConfigMaps?</summary><br><b>

1. Создайте его (из ключа и значения, файла или env-файла)
2. Присоедините его. Смонтируйте configmap как том.
</b></details>

<details>
<summary>Верно или Ложно? Конфиденциальные данные, такие как учетные данные, должны храниться в ConfigMap</summary><br><b>

Ложно. Используйте секрет.
</b></details>

<details>
<summary>Объясните "Горизонтальное автоматическое масштабирование подов"</summary><br><b>

В Kubernetes "Горизонтальное автоматическое масштабирование подов" автоматически обновляет рабочий ресурс с целью автоматически масштабировать рабочую нагрузку, чтобы соответствовать спросу.
</b></details>

<details>
<summary>Когда вы удаляете под, удаляется ли он мгновенно? (через мгновение после выполнения команды)</summary><br><b>
</b></details>

<details>
<summary>Что означает облачная нативность?</summary><br><b>
  Термин облачная нативность относится к концепции построения и запуска приложений, чтобы использовать преимущества распределенных вычислений, предлагаемых моделью доставки облака.
</b></details>

<details>
<summary>Объясните подход "домашний питомец и скот" в инфраструктуре в отношении Kubernetes</summary><br><b>
</b></details>

<details>
<summary>Опишите, как вы добьётесь запуска контейнерного веб-приложения в K8s, которое должно быть доступно по общедоступному URL.</summary><br><b>
</b></details>

<details>
<summary>Как бы вы устранили неполадки кластера, если некоторые приложения больше не доступны?</summary><br><b>
</b></details>

<details>
<summary>Опишите, какие определения пользовательских ресурсов существуют в мире Kubernetes? Для чего они могут использоваться?</summary><br><b>
</b></details>

<details>
<summary> Как работает планирование в Kubernetes?</summary><br><b>

Контрольный компонент kube-scheduler задает следующие вопросы:
1. Что планировать? Он пытается понять спецификации определения пода.
2. Какой узел планировать? Он пытается определить лучший узел с доступными ресурсами для создания пода.
3. Привязка Пода к данному узлу.

Узнайте больше [здесь](https://www.youtube.com/watch?v=rDCWxkvPlAw).
</b></details>

<details>
<summary> Как используются метки и селекторы?</summary><br><b>
</b></details>

<details>
<summary>Какие классы QoS существуют?</summary><br><b>

* Guarantee
* Burstable
* BestEffort
</b></details>

<details>
<summary>Объясните метки. Что это и зачем они нужны?</summary><br><b>
  
Метки Kubernetes являются ключевыми парами-значениями, которые могут соединять идентифицирующуюMetadata с объектами Kubernetes.
</b></details>

<details>
<summary>Объясните селекторы</summary><br><b>
</b></details>

<details>
<summary>Что такое Kubeconfig?</summary><br><b>
</b></details>

### Gatekeeper

<details>
<summary>Что такое Gatekeeper?</summary><br><b>

[Документация по Gatekeeper](https://open-policy-agent.github.io/gatekeeper/website/docs): "Gatekeeper — это валидирующий (мутационный TBA) веб-хук, который обеспечивает соблюдение политик на основе CRD, выполняемых Открытым Политическим Агентом."
</b></details>

<details>
<summary>Объясните, как работает Gatekeeper</summary><br><b>

На каждый запрос, отправленный в кластер Kubernetes, Gatekeeper отправляет политики и ресурсы в OPA (Open Policy Agent), чтобы проверить, нарушает ли он какие-либо политики. Если это так, Gatekeeper вернет сообщение об ошибке политики. Если политика не нарушена, запрос дойдет до кластера.
</b></details>

### Тестирование политик

<details>
<summary>Что такое Conftest?</summary><br><b>

Conftest позволяет вам писать тесты для структурированных файлов. Вы можете подумать о нем как о библиотеке тестов для ресурсов Kubernetes.<br>
Он в основном используется в тестовых средах, таких как CI-процессы или локальные хуки.
</b></details>

<details>
<summary>Что такое Datree? Чем он отличается от Conftest?</summary><br><b>

Так же как и Conftest, он используется для тестирования и соблюдения политик. Разница в том, что он поставляется с встроенными политиками.
</b></details>


### Безопасность

<details>
<summary>Какие лучшие практики безопасности вы соблюдаете в отношении кластера Kubernetes?</summary><br><b>

  * Защитите межслужебное общение (один из способов — использовать Istio для предоставления взаимного TLS).
  * Изолируйте различные ресурсы в отдельные пространства имен на основании некоторых логических групп.
  * Используйте поддерживаемый контейнерный движок (если вы используете Docker, отвергайте его, так как он устарел. Есть смысл рассмотреть CRI-O как движок и podman для CLI).
  * Правильно тестируйте изменения в кластере (например, подумайте о Datree для предотвращения неправильных конфигураций Kubernetes).
  * Ограничьте, кто может что делать (например, с помощью OPA Gatekeeper) в кластере.
  * Используйте сетевую политику для применения сетевой безопасности.
  * Рассмотрите возможность использования инструментов (например, Falco) для мониторинга угроз.
</b></details>

### Сценарии устранения неполадок

<details>
<summary>Выполнив <code>kubectl get pods</code>, вы видите Поды в статусе "Pending". Что бы вы сделали?</summary><br><b>

Один из возможных путей — выполнить `kubectl describe pod <pod name>`, чтобы получить больше деталей.<br>
Вы можете увидеть одну из следующих причин:
  * Кластер переполнен. В этом случае расширьте кластер.
  * Достигнуты лимиты квоты ресурсов. В этом случае вам, возможно, нужно будет их изменить.
  * Проверьте, если монтирование PersistentVolumeClaim приостановлено.

Если ничего из вышеперечисленного не помогло, выполните команду (`get pods`) с `-o wide`, чтобы увидеть, назначен ли узел. Если нет, возможно, существует проблема с планировщиком.
</b></details>

<details>
<summary>Пользователи не могут получить доступ к приложению, работающему на поде Kubernetes. В чем может быть проблема и как это проверить?</summary><br><b>

Один из возможных путей — начать с проверки статуса Подов.
1. Под в ожидании? Если да, проверьте, в чем причина с помощью `kubectl describe pod <pod name>`.
TODO: завершить это...
</b></details>

### Контроллеры

<details>
<summary>Что такое контроллеры?</summary><br><b>

[Kubernetes.io](https://kubernetes.io/docs/concepts/architecture/controller): "В Kubernetes контроллеры — это контрольные петли, которые наблюдают за состоянием вашего кластера и затем вносят или запрашивают изменения, если это необходимо. Каждый контроллер пытается приблизить текущее состояние кластера к желаемому состоянию."
</b></details>

<details>
<summary>Назовите два контроллера, с которыми вы знакомы</summary><br><b>

1. Контроллер Узлов: управляет узлами кластера. Среди прочего, этот контроллер отвечает за мониторинг здоровья узлов — если узел внезапно становится недоступным, он эвакуирует все поды, работающие на нем, и отмечает статус узла соответствующим образом.
2. Контроллер Репликации - мониторит состояние реплик подов, исходя из того, что должно работать. Он следит за тем, чтобы количество подов, которое должно работать, действительно работало.
</b></details>

<details>
<summary>Какой процесс отвечает за запуск и установку различных контроллеров?</summary><br><b>

Kube-Controller-Manager
</b></details>

<details>
<summary>Что такое контрольная петля? Как она работает?</summary><br><b>

Объяснено [здесь](https://www.youtube.com/watch?v=i9V4oCa5f9I).
</b></details>

<details>
<summary>Каковы все фазы/этапы контрольной петли?</summary><br><b>

- Наблюдение - определить текущее состояние кластера.
- Diff - определить, существует ли дифф между текущим состоянием и желаемым состоянием.
- Действие - привести текущее состояние кластера к желаемому состоянию (в основном достичь состояния, когда нет различий).
</b></details>

### Планировщик

<details>
<summary>Верно или Ложно? Планировщик отвечает за решение, где будет запущен Под, и за его фактический запуск</summary><br><b>

Ложно. Хотя планировщик отвечает за выбор узла, на котором будет работать Под, Kubelet — это тот, кто фактически запускает Под.
</b></details>

<details>
<summary>Как запланировать под на узле с названием "node1"?</summary><br><b>

`k run some-pod --image=redix -o yaml --dry-run=client > pod.yaml`

`vi pod.yaml` и добавьте:

```
spec:
  nodeName: node1
```

`k apply -f pod.yaml`

Примечание: если у вас нет узла с именем node1 в вашем кластере, Под останется в состоянии "Pending".
</b></details>

#### Аффинность узла

<details>
<summary>Используя аффинность узла, установите Под на запланированного узла, где ключ "region" и значение "asia" или "emea"</summary><br><b>

`vi pod.yaml`

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: region
          operator: In
          values:
          - asia
          - emea
```
</b></details>

<details>
<summary>Используя аффинность узла, установите Под, чтобы он никогда не планировался на узел, где ключ "region" и значение "neverland"</summary><br><b>

`vi pod.yaml`

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: region
          operator: NotIn
          values:
          - neverland
```
</b></details>

<details>
<summary>Верно или Ложно? Использование аффинности узла типа "requiredDuringSchedulingIgnoredDuringExecution" означает, что планировщик не может запланировать, если правило не выполнено</summary><br><b>

Верно
</b></details>

<details>
<summary>Верно или Ложно? Использование аффинности узла типа "preferredDuringSchedulingIgnoredDuringExecution" означает, что планировщик не может запланировать, если правило не выполнено</summary><br><b>

Ложно. Планировщик пытается найти узел, который соответствует требованиям/правилам, и если не удается, он все равно запланирует Под.
</b></details>

<details>
<summary>Можно ли развернуть несколько планировщиков?</summary><br><b>

Да, это возможно. Вы можете запустить другой под с командой, похожей на:

```
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --leader-elect=true
    - --scheduler-name=some-custom-scheduler
...
```
</b></details>

<details>
<summary>Предположим, у вас несколько планировщиков, как узнать, какой планировщик использовался для данного Под?</summary><br><b>

Запустив `kubectl get events`, вы сможете увидеть, какой планировщик был использован.
</b></details>

<details>
<summary>Вы хотите запустить новый Под и хотите, чтобы он был запланирован с помощью нестандартного планировщика. Как это сделать?</summary><br><b>

Добавьте следующее в спецификацию Пода:

```
spec:
  schedulerName: some-custom-scheduler
```
</b></details>

### Модификаторы

<details>
<summary>Проверьте, есть ли деформации на узле "master"</summary><br><b>

`k describe no master | grep -i taints`
</b></details>

<details>
<summary>Создайте деформацию на одном из узлов в вашем кластере с ключом "app" и значением "web" и эффектом "NoSchedule". Проверьте, была ли она применена</summary><br><b>

`k taint node minikube app=web:NoSchedule`

`k describe no minikube | grep -i taints`
</b></details>

<details>
<summary>Вы применили деформацию с <code>k taint node minikube app=web:NoSchedule</code> на единственном узле в вашем кластере, а затем выполнили <code>kubectl run some-pod --image=redis</code>. Что произойдет?</summary><br><b>

Под останется в статусе "Pending" из-за того, что единственный узел в кластере имеет деформацию "app=web".
</b></details>

<details>
<summary>Вы применили деформацию с <code>k taint node minikube app=web:NoSchedule</code> на единственном узле в вашем кластере, а затем выполнили <code>kubectl run some-pod --image=redis</code>, но Под находится в ожидании. Как это исправить?</summary><br><b>

`kubectl edit po some-pod` и добавьте следующее

```
  - effect: NoSchedule
    key: app
    operator: Equal
    value: web
```

Выйдите и сохраните. Теперь под должен находиться в состоянии "Running".
</b></details>

<details>
<summary>Удалите существующую деформацию с одного из узлов в вашем кластере</summary><br><b>

`k taint node minikube app=web:NoSchedule-`
</b></details>

<details>
<summary>Какие эффекты деформации существуют? Объясните каждый из них</summary><br><b>

`NoSchedule`: предотвращает запланирование ресурсов на определённом узле.
`PreferNoSchedule`: предпочтительно запланировать ресурсы на других узлах прежде, чем прибегнуть к планированию ресурса на выбранном узле (на который была применена деформация).
`NoExecute`: Применение "NoSchedule" не исключает уже работающие поды (или другие ресурсы) с узла, в отличие от "NoExecute", который исключает любые уже работающие ресурсы с узла.
</b></details>

### Ограничения ресурсов

<details>
<summary>Объясните, почему было бы указать ограничения ресурсов в отношении Подов</summary><br><b>

* Вы знаете, сколько ОЗУ и/или ЦП ваше приложение должно потреблять, и любое большее значение недопустимо.
* Вы хотите убедиться, что все могут запускать свои приложения в кластере, и ресурсы не используются исключительно одним типом приложения.
</b></details>

<details>
<summary>Верно или Ложно? Ограничения ресурсов применяются на уровне Подов, что означает, что если лимит составляет 2 ГБ ОЗУ, а в Поде два контейнера, то это 1 ГБ ОЗУ для каждого</summary><br><b>

Ложно. Это на контейнер и не на Под.
</b></details>

#### Команды Ограничений Ресурсов

<details>
<summary>Как проверить, есть ли ограничения на одном из подов в вашем кластере?</summary><br><b>

`kubectl describe po <POD_NAME> | grep -i limits`
</b></details>

<details>
<summary>Запустите под с названием "yay" с образом "python" и запросами ресурсов 64Mi памяти и 250m ЦП</summary><br><b>

`kubectl run yay --image=python --dry-run=client -o yaml > pod.yaml`

`vi pod.yaml`

```
spec:
  containers:
  - image: python
    imagePullPolicy: Always
    name: yay
    resources:
      requests:
        cpu: 250m
        memory: 64Mi
```

`kubectl apply -f pod.yaml`
</b></details>

<details>
<summary>Запустите под с названием "yay2" с образом "python". Убедитесь, что он имеет запросы ресурсов на 64Mi ОЗУ и 250m ЦП и ограничения на 128Mi ОЗУ и 500m ЦП</summary><br><b>

`kubectl run yay2 --image=python --dry-run=client -o yaml > pod.yaml`

`vi pod.yaml`

```
spec:
  containers:
  - image: python
    imagePullPolicy: Always
    name: yay2
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 64Mi
```

`kubectl apply -f pod.yaml`
</b></details>

### Мониторинг

<details>
<summary>С какими решениями для мониторинга вы знакомы в отношении Kubernetes?</summary><br><b>

Существует множество различных решений для мониторинга Kubernetes. Некоторые из них с открытым исходным кодом, некоторые в памяти, некоторые стоят денег... вот короткий список:

* metrics-server: мониторинг с открытым исходным кодом с хранением в памяти.
* datadog: $$$
* promethues: решение для мониторинга с открытым исходным кодом.
</b></details>

<details>
<summary>Опишите, как ваше решение мониторинга контролирует Kubernetes</summary><br><b>

Это в значительной степени зависит от того, что вы выбрали. Рассмотрим некоторые решения:

* metrics-server: это бесплатное решение с открытым исходным кодом, использующее компонент cAdvisor от kubelet для получения информации о кластере и его ресурсах и сохранения их в памяти.
После установки, через некоторое время вы можете выполнить такие команды, как `kubectl top node` и `kubectl top pod`, чтобы увидеть показатели производительности узлов, подов и других ресурсов.

TODO: добавить другие решения для мониторинга
</b></details>

### Kustomize

<details>
<summary>Что такое Kustomize?</summary><br><b>
</b></details>

<details>
<summary>Объясните необходимость Kustomize, описав реальные случаи использования</summary><br><b>

* У вас есть helm chart приложения, используемого несколькими командами в вашей организации, и есть требование добавить аннотации к приложению, указывающим имя команды, владеющей приложением.
  * Без Kustomize вам нужно будет копировать файлы (шаблон чарта в этом случае) и изменять их, чтобы включить конкретные необходимые аннотации.
  * С Kustomize вам не нужно копировать весь репозиторий или файлы.
* Вам нужно применить изменение/патч к некоторому приложению, не изменяя оригинальные файлы приложения.
  * С Kustomize вы можете определить файл kustomization.yml, который определяет эти настройки, так что вам не нужно прикасаться к оригинальным файлам приложения.
</b></details>

<details>
<summary>Опишите, как работает Kustomize на высоком уровне</summary><br><b>

1. Вы добавляете файл kustomization.yml в папку приложения, которую хотите изменить.
   1. Вы определяете настройки, которые хотите выполнить.
2. Вы выполняете `kustomize build APP_PATH`, где также находится ваш файл kustomization.yml.
</b></details>

### Стратегии развертывания

<details>
<summary>Какие стратегии развертывания/версии вам известны?</summary><br><b>

* Развертывания Blue/Green: вы развертываете новую версию вашего приложения, в то время как старая версия все еще работает, и вы начинаете перенаправлять трафик на новую версию приложения.
* Канарейка: вы развертываете новую версию вашего приложения и начинаете перенаправлять **часть** ваших пользователей/трафика на новую версию. Таким образом, миграция на новую версию более постепенная.
</b></details>

<details>
<summary>Подробно объясните развертывания Blue/Green</summary><br><b>

Шаги развертывания Blue/Green:

1. Трафик, исходящий от пользователей через балансировщик нагрузки к приложению, которое в настоящее время версия 1.

Пользователи -> Балансировщик нагрузки -> Версия приложения 1

2. Новая версия приложения 2 развертывается (в то время как версия 1 все еще работает).

Пользователи -> Балансировщик нагрузки -> Версия приложения 1
                          Версия приложения 2

3. Если версия 2 работает правильно, трафик переключается на нее вместо версии 1.

Пользователи -> Балансировщик нагрузки     Версия приложения 1
                       -> Версия приложения 2

4. Удаляется ли старая версия или остается работающей, но без перенаправления пользователей на нее — это зависит от решения команды или компании.

Плюсы:
  * Мы можем быстро откатиться/переключиться на предыдущую версию в любое время.
Минусы:
  * В случае проблемы с новой версией все пользователи подвержены риску (в отличие от небольшой части/процента).
</b></details>

<details>
<summary>Подробно объясните развертывание Канарейка</summary><br><b>

Шаги развертывания Канарейка:

1. Трафик, исходящий от пользователей через балансировщик нагрузки к приложению, которое в настоящее время версия 1.

Пользователи -> Балансировщик нагрузки -> Версия приложения 1

2. Новая версия приложения 2 развертывается (в то время как версия 1 все еще работает) и часть трафика перенаправляется на новую версию.

Пользователи -> Балансировщик нагрузки ->(95% трафика) Версия приложения 1
                       ->(5% трафика) Версия приложения 2

3. Если новая версия (2) работает хорошо, больше трафика перенаправляется на нее.

Пользователи -> Балансировщик нагрузки ->(70% трафика) Версия приложения 1
                       ->(30% трафика) Версия приложения 2

3. Если все работает хорошо, в какой-то момент весь трафик перенаправляется на новую версию.

Пользователи -> Балансировщик нагрузки -> Версия приложения 2


Плюсы:
  * Если возникнет какая-либо проблема с новой развернутой версией приложения, только некоторые пользователи подвергаются риску, а не все из них.
Минусы:
  * Тестирование новой версии необходимо в производственной среде (так как пользовательский трафик существует только там).
</b></details>

<details>
<summary>Какие способы вам известны для реализации стратегий развертывания (например, канарейка, blue/green) в Kubernetes?</summary><br><b>

Существует несколько способов. Один из них - Argo Rollouts.
</b></details>

### Сценарии

<details>
<summary>Инженер из вашей организации сказал вам, что он заинтересован только в том, чтобы видеть ресурсы своей команды в Kubernetes. На самом деле, он видит ресурсы всей организации, из нескольких разных команд. Какую концепцию Kubernetes вы можете использовать, чтобы с этим разобраться?</summary><br><b>

Пространства имен (Namespaces). См. следующий [вопрос о пространствах имен и ответ](#namespaces-use-cases) для получения дополнительной информации.
</b></details>

<details>
<summary>Инженер в вашей команде запустил Pod, но статус, который он видит, - "CrashLoopBackOff". Что это означает? Как определить проблему?</summary><br><b>

Контейнер не смог запуститься (по различным причинам), и Kubernetes пытается запустить Pod снова после некоторой задержки (= время BackOff).

Некоторые причины, по которым он может не запуститься:
  - Неправильная конфигурация - опечатка, неподдерживаемое значение и т.д.
  - Ресурс недоступен - узлы отключены, PV не смонтирован и т.д.

Некоторые способы диагностики:

1. `kubectl describe pod POD_NAME`
   1. Обратите внимание на `State` (который должен быть Waiting или CrashLoopBackOff) и `Last State`, который должен показать, что произошло ранее (почему он не смог запуститься)
2. Выполните `kubectl logs mypod`
   1. Это должно предоставить точный вывод о 
   2. Для конкретного контейнера вы можете добавить `-c CONTAINER_NAME`
</b></details>

<details>
<summary>Инженер из вашей организации спросил, есть ли способ предотвратить запуск Pod (с определенной меткой) на одном из узлов кластера. Ваш ответ:</summary><br><b>

Да, используя taints, мы можем выполнить следующую команду, и это предотвратит запуск всех ресурсов с меткой "app=web" на node1: `kubectl taint node node1 app=web:NoSchedule`
</b></details>

<details>
<summary>Вы хотите ограничить количество ресурсов, используемых в вашем кластере. Например, не более 4 ReplicaSets, 2 Services и т.д. Как бы вы этого добились?</summary><br><b>

Используя ResourceQuotas.
</b></details>
